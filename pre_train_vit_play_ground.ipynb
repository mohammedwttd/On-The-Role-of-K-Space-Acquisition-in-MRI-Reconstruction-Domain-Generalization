{
 "cells": [
  {
   "cell_type": "code",
   "id": "7fec237e3433d522",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import torch;\n",
    "\n",
    "from train import normalize\n",
    "\n",
    "print(torch.__version__)\n",
    "import torchvision; print(torchvision.__version__)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "16b93803fdb7afe8",
   "metadata": {},
   "source": [
    "# pytorch\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as tvtransformss\n",
    "\n",
    "# fastmri\n",
    "import fastmri\n",
    "from fastmri.data import subsample\n",
    "from fastmri.data import transforms, mri_data\n",
    "from fastmri.evaluate import ssim, psnr, nmse\n",
    "from fastmri.losses import SSIMLoss\n",
    "from fastmri.models import Unet\n",
    "\n",
    "# other\n",
    "import random\n",
    "import PIL.Image as Image\n",
    "from glob import glob\n",
    "from myutils import SSIM, PSNR\n",
    "from models.rec_models.vit_model import VisionTransformer\n",
    "from models.rec_models.recon_net import ReconNet\n",
    "\n",
    "# Device\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "device = 'cuda'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "110e91c1e159f24a",
   "metadata": {},
   "source": [
    "print(\"hello\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4355a52d-2322-4e1a-a511-3cc6c5d1ec31",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torchvision.transforms import Resize, ToPILImage\n",
    "%matplotlib inline\n",
    "def show_image(source):\n",
    "    try:\n",
    "        source = source.clone()\n",
    "        source.reshape(320, 320)\n",
    "        image = source\n",
    "        image -= image.min()\n",
    "        max_val = image.max()\n",
    "        if max_val > 0:\n",
    "            image /= max_val\n",
    "        source = image\n",
    "        grid = torchvision.utils.make_grid(source, nrow=4, pad_value=1)\n",
    "        numpy_image = grid.permute(1, 2, 0).cpu().detach().numpy()\n",
    "    \n",
    "        # Save or display the image\n",
    "        plt.imshow(numpy_image)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    except:\n",
    "        return"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56a51ea0a14e185b",
   "metadata": {},
   "source": [
    "class fastMRIDataset(Dataset):\n",
    "    def __init__(self, challenge, path, isval, sample_rate = 0.1):\n",
    "        \"\"\"\n",
    "        Dataloader for 4x acceleration and random sampling\n",
    "        challenge: 'multicoil' or 'singlecoil'\n",
    "        path: path to dataset\n",
    "        isval: whether dataset is fastMRI's validation set or training set\n",
    "        \"\"\"\n",
    "        self.challenge = challenge \n",
    "        self.data_path = path\n",
    "        self.isval = isval\n",
    "\n",
    "        self.data = mri_data.SliceDataset(\n",
    "            root=self.data_path,\n",
    "            transform=self.data_transform,\n",
    "            challenge=self.challenge,\n",
    "            use_dataset_cache=True,\n",
    "            sample_rate = sample_rate\n",
    "            )\n",
    "\n",
    "        self.mask_func = subsample.EquispacedMaskFunc( # RandomMaskFunc for knee, EquispacedMaskFunc for brain\n",
    "            center_fractions=[0.08],\n",
    "            accelerations=[4],\n",
    "            )\n",
    "            \n",
    "    def data_transform(self, kspace, mask, target, data_attributes, filename, slice_num):\n",
    "        if self.isval:\n",
    "            seed = tuple(map(ord, filename))\n",
    "        else:\n",
    "            seed = None     \n",
    "        kspace = transforms.to_tensor(kspace)\n",
    "        masked_kspace, _ = transforms.apply_mask(kspace, self.mask_func, seed)        \n",
    "        \n",
    "        target = transforms.to_tensor(target)\n",
    "        zero_fill = fastmri.ifft2c(masked_kspace)\n",
    "        zero_fill = transforms.complex_center_crop(zero_fill, target.shape)   \n",
    "        x_zero_fill = fastmri.complex_abs(zero_fill)\n",
    "        \n",
    "        if self.challenge == 'multicoil':\n",
    "            x_zero_fill = fastmri.rss(x_zero_fill)\n",
    "            zero_fill = fastmri.rss(zero_fill)\n",
    "\n",
    "        x_zero_fill = x_zero_fill.unsqueeze(0)\n",
    "        zero_fill = zero_fill.unsqueeze(0)\n",
    "        target = target.unsqueeze(0)\n",
    "        \n",
    "        return (x_zero_fill, target, data_attributes['max'], zero_fill)    \n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "\n",
    "        return data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3d71c175-22ab-4060-93ed-0ba2ab45a975",
   "metadata": {},
   "source": [
    "challenge = 'multicoil' # 'multicoil' or 'singlecoil'\n",
    "train_path = '/mnt/walkure_public/users/mohammedw/fastmri_downloads/multicoil_train/' # path to fastmri's training data\n",
    "val_path = '/mnt/walkure_public/users/mohammedw/fastmri_downloads/multicoil_val/' # path to fastmri's validation data\n",
    "dataset = fastMRIDataset(challenge=challenge, path=train_path, isval=False)\n",
    "val_dataset = fastMRIDataset(challenge=challenge, path=val_path, isval=True)\n",
    "\n",
    "ntrain = len(dataset) # number of training data\n",
    "train_dataset, _ = torch.utils.data.random_split(dataset, [ntrain, len(dataset)-ntrain], generator=torch.Generator().manual_seed(42))\n",
    "print(len(train_dataset))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9b7658a4-0201-4111-aca6-685c55f3613e",
   "metadata": {},
   "source": [
    "batch_size = 1\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, generator=torch.Generator().manual_seed(42))\n",
    "valloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)  "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18218b70-433c-46a7-b0fe-ac4300f75a24",
   "metadata": {},
   "source": [
    "print(len(valloader))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17676839-558f-4447-afda-35a7fefe293b",
   "metadata": {},
   "source": [
    "trainloader"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3772be12-dfa7-4ae3-bed6-aae4b605b8f8",
   "metadata": {},
   "source": [
    "# Validate model\n",
    "def validate(model):\n",
    "    valloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)   \n",
    "    model.eval()    \n",
    "    ssim_ = SSIM().to(device)\n",
    "    psnr_ = PSNR().to(device)\n",
    "    psnrs = []\n",
    "    ssims = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            inputs, targets, maxval, _ = data\n",
    "            inputs = normalize(center_crop(inputs, 320, 320))\n",
    "            targets = normalize(center_crop(targets, 320, 320))\n",
    "            outputs = model(inputs.to(device))\n",
    "            show_image(outputs)\n",
    "            ssims.append(ssim_(outputs, targets.to(device), maxval.to(device)))\n",
    "            psnrs.append(psnr_(outputs, targets.to(device), maxval.to(device)))\n",
    "    \n",
    "    ssimval = torch.cat(ssims).mean()\n",
    "    \n",
    "    print(' Recon. PSNR: {:0.3f} pm {:0.2f}'.format(torch.cat(psnrs).mean(), 2*torch.cat(psnrs).std()))\n",
    "    print(' Recon. SSIM: {:0.4f} pm {:0.3f}'.format(torch.cat(ssims).mean(), 2*torch.cat(ssims).std()))\n",
    "                \n",
    "    return (1-ssimval).item()\n",
    "\n",
    "# Save model\n",
    "def save_model(path, model, train_hist, val_hist, optimizer, scheduler=None):\n",
    "    net = model.net\n",
    "    if scheduler:\n",
    "        checkpoint = {\n",
    "            'model' :  ReconNet(net),\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(), \n",
    "        }\n",
    "    else:\n",
    "        checkpoint = {\n",
    "            'model' :  ReconNet(net),\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }\n",
    "        \n",
    "    torch.save(train_hist, path + 'train_hist.pt')\n",
    "    torch.save(val_hist, path + 'val_hist.pt')    \n",
    "    torch.save(checkpoint,  path + 'checkpoint.pth')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b3dcef35f2158b90",
   "metadata": {},
   "source": [
    "# Step 1: Re-initialize everything\n",
    "avrg_img_size = 340\n",
    "patch_size = 10\n",
    "depth = 10\n",
    "num_heads = 16\n",
    "embed_dim = 44\n",
    "\n",
    "net = VisionTransformer(\n",
    "    avrg_img_size=avrg_img_size,\n",
    "    patch_size=patch_size,\n",
    "    in_chans=1, embed_dim=embed_dim,\n",
    "    depth=depth, num_heads=num_heads,\n",
    "    )\n",
    "\n",
    "model = ReconNet(net)  # replace `net` as needed\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # match optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # match scheduler\n",
    "\n",
    "path = \"models/vit-l_equidist_acc4/checkpoint.pth\"\n",
    "# Step 2: Load the checkpoint\n",
    "checkpoint = torch.load(path)\n",
    "\n",
    "# Step 3: Restore state dicts\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "# Step 4: Set model to eval or train\n",
    "model.train()  # or model.train() depending on usage"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c8679e8f-1f91-4f86-886a-66145cfacdf8",
   "metadata": {},
   "source": [
    "\"\"\"Optimizer\"\"\"\n",
    "criterion = torch.nn.L1Loss() #CompositeMRILoss(15, 0.0025, 0.1).to(device) #torch.nn.MSELoss() #torch.nn.L1Loss()  #SSIMLoss().to(device) #torch.nn.L1Loss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=2 * 0.0001)\n",
    "train_hist = []\n",
    "val_hist = []\n",
    "best_val = float(\"inf\")\n",
    "path = './' # Path for saving model checkpoint and loss history\n",
    "num_epochs = 100000\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.0003,\n",
    "                                          total_steps=num_epochs, pct_start=0.1,\n",
    "                                          anneal_strategy='linear',\n",
    "                                          cycle_momentum=False,\n",
    "                                          base_momentum=0., max_momentum=0., div_factor=0.1*num_epochs, final_div_factor=9)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d5e79ce-b797-4b21-8b3a-ddd537d23bbf",
   "metadata": {},
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = 0.00001,              # Learning rate\n",
    "    betas=(0.9, 0.999),   # Default Adam settings\n",
    "    eps=1e-8,             # For numerical stability\n",
    "    weight_decay=0        # Set to 0 for overfitting (no regularization)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1948541b4babde01",
   "metadata": {},
   "source": [
    "def center_crop(tensor, *target_sizes):\n",
    "    \"\"\"\n",
    "    Center crop the last N dimensions to match target_sizes.\n",
    "    \"\"\"\n",
    "    spatial_dims = tensor.shape[-len(target_sizes):]\n",
    "    starts = [(dim - target) // 2 for dim, target in zip(spatial_dims, target_sizes)]\n",
    "    slices = tuple(slice(start, start + size) for start, size in zip(starts, target_sizes))\n",
    "    return tensor[(...,) + slices]\n",
    "\n",
    "def top_left_crop(tensor, *target_sizes):\n",
    "    \"\"\"\n",
    "    Top-left crop the last N dimensions to match target_sizes.\n",
    "    \"\"\"\n",
    "    slices = tuple(slice(0, size) for size in target_sizes)\n",
    "    return tensor[(...,) + slices]\n",
    "\n",
    "def crop_at(tensor, starts, sizes):\n",
    "    \"\"\"\n",
    "    Crop starting at 'starts' with 'sizes' over the last N dimensions.\n",
    "    starts: tuple of starting indices (length = num spatial dims)\n",
    "    sizes: tuple of sizes (length = num spatial dims)\n",
    "    \"\"\"\n",
    "    slices = tuple(slice(start, start + size) for start, size in zip(starts, sizes))\n",
    "    return tensor[(...,) + slices]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aae64d2c-2cc5-4dbe-b87e-3cde62ff466b",
   "metadata": {},
   "source": [
    "validate(model.to(\"cuda\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9bd93d29-9452-439e-90cb-0c6c726177c4",
   "metadata": {},
   "source": [
    "\"\"\"Train Model\"\"\"\n",
    "from common.evaluate import psnr, ssim\n",
    "from train import normalize\n",
    "model = model.to(\"cuda\")\n",
    "for epoch in range(0, 10):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for iter, data in enumerate(valloader):\n",
    "        inputs, targets, maxval, inputs_c = data\n",
    "        #inputs_c = inputs_c.squeeze(0).permute(0, 3, 1, 2)\n",
    "        inputs = (center_crop(inputs, 320, 320))\n",
    "        targets = (center_crop(targets, 320, 320))\n",
    "        #inputs_c = center_crop(inputs_c, 2, 320, 320)\n",
    "        #print(inputs_c, targets)\n",
    "        optimizer.zero_grad()\n",
    "        print(\"1\")\n",
    "        #outputs = fastmri.complex_abs(model(inputs_c.to(device)).permute(0, 2, 3, 1))\n",
    "        #print(output)\n",
    "        outputs = model(inputs.to(device))\n",
    "        #print(outputs.shape)\n",
    "        show_image(inputs[0])\n",
    "        show_image(outputs[0])\n",
    "        loss = criterion(outputs, targets.to(device))#, maxval.to(device))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1, norm_type=1.)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "            \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "        \n",
    "    train_hist.append(train_loss/len(trainloader))\n",
    "    print('Epoch {}, Train loss.: {:0.10f}'.format(epoch+1, train_hist[-1]))\n",
    "    \n",
    "    if (epoch+1)%5==0:\n",
    "        print('Validation:')\n",
    "        val_hist.append(validate(model))        \n",
    "        if val_hist[-1] < best_val:\n",
    "            save_model(path, model, train_hist, val_hist, optimizer, scheduler=scheduler)\n",
    "            best_val = val_hist[-1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "460f9f81-0478-4e15-9456-cb97c7a66a33",
   "metadata": {},
   "source": [
    "print(\"hello\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9cf6602cb3a25c73",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
