{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Attcking notebook",
   "id": "51b9019c6b00607e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import logging\n",
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "import sys\n",
    "# sys.path.insert(0, '/home/tomerweiss/multiPILOT2')\n",
    "from tqdm import tqdm  \n",
    "import numpy as np\n",
    "# np.seterr('raise')\n",
    "import torch\n",
    "import torchvision\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from common.args import Args\n",
    "from data import transforms\n",
    "from data.mri_data import SliceData\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from models.subsampling_model import Subsampling_Model\n",
    "from scipy.spatial import distance_matrix\n",
    "from tsp_solver.greedy import solve_tsp\n",
    "import scipy.io as sio\n",
    "from common.utils import get_vel_acc\n",
    "from common.evaluate import psnr, ssim\n",
    "from fastmri.losses import SSIMLoss\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "ac355fe450fc2433"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import fastmri.models\n",
    "from models.rec_models.unet_model import UnetModel\n",
    "from models.rec_models.complex_unet import ComplexUnetModel\n",
    "import data.transforms as transforms\n",
    "from pytorch_nufft.nufft import nufft, nufft_adjoint\n",
    "import numpy as np\n",
    "from WaveformProjection.run_projection import proj_handler\n",
    "import matplotlib.pylab as P\n",
    "from models.rec_models.vision_transformer import VisionTransformer\n",
    "from models.rec_models.recon_net import ReconNet\n",
    "from models.rec_models.humus_net import HUMUSNet, HUMUSBlock\n",
    "from  models.VarBlock import VarNet\n",
    "from typing import Tuple\n",
    "from fastmri.data.subsample import create_mask_for_mask_type\n",
    "from fastmri.data.transforms import apply_mask\n",
    "\n",
    "import torch\n",
    "import numpy as np\n"
   ],
   "id": "92607383d2ce5b7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA device not available\")"
   ],
   "id": "fb2481c222ce61c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.cuda.empty_cache()",
   "id": "e936f96a128718a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DataTransform:\n",
    "    def __init__(self, resolution):\n",
    "        self.resolution = resolution\n",
    "\n",
    "    def __call__(self, kspace, target, attrs, fname, slice):\n",
    "        kspace = transforms.to_tensor(kspace)\n",
    "        #print(\"kspace: \",kspace.shape)\n",
    "        image = transforms.ifft2_regular(kspace)\n",
    "        image = transforms.complex_center_crop(image, (self.resolution, self.resolution))\n",
    "        image, mean, std = transforms.normalize_instance(image, eps=1e-11)\n",
    "        target = transforms.to_tensor(target)\n",
    "        target, mean, std = transforms.normalize_instance(target, eps=1e-11)\n",
    "        # # target = transforms.normalize(target, mean, std)\n",
    "        # target = target.clamp(-6, 6)\n",
    "        mean = std = 0\n",
    "\n",
    "        if target.shape[1] != self.resolution:\n",
    "            target = transforms.center_crop(target, (self.resolution, self.resolution))\n",
    "        return image.mean(0) , target, mean, std, attrs['norm'].astype(np.float32)\n",
    "\n",
    "\n",
    "def create_test_dataset(args):\n",
    "    test_path = args.data_path / 'multicoil_val' \n",
    "    test_data = SliceData(\n",
    "        root=test_path,\n",
    "        transform=DataTransform(args.resolution),\n",
    "        sample_rate=args.sample_rate)\n",
    "    return test_data\n",
    "\n",
    "\n",
    "def create_test_loader(args):\n",
    "    test_data = create_test_dataset(args)\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=20,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return test_loader"
   ],
   "id": "f7d88be6376f851a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate(args, model, data_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    psnr_l = []\n",
    "    ssim_l = []\n",
    "    progress_bar = tqdm(enumerate(data_loader), total=len(data_loader), desc=\"Processing Batches\")\n",
    "    for iter, data in progress_bar:\n",
    "        input, target, mean, std, norm = data\n",
    "\n",
    "        # Move input and target to the specified device\n",
    "        input = input.to(args.device)\n",
    "        target = target.to(args.device)\n",
    "\n",
    "        # Model prediction\n",
    "        output = model(input.unsqueeze(1))\n",
    "\n",
    "        # Reconstruction\n",
    "        recons = output.to('cpu').squeeze(1).view(target.shape)\n",
    "        recons = recons.squeeze()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.l1_loss(output.squeeze(), target)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Reshape reconstructions\n",
    "        recons = recons.view(target.shape)\n",
    "\n",
    "        # Compute PSNR and SSIM\n",
    "        psnr_value = psnr(target.detach().to('cpu').numpy(), recons.detach().numpy())\n",
    "        ssim_value = ssim(target.detach().to('cpu').numpy(), recons.detach().numpy())\n",
    "        psnr_l.append(psnr_value)\n",
    "        ssim_l.append(ssim_value)\n",
    "\n",
    "        # Update the tqdm progress bar with metrics\n",
    "        progress_bar.set_postfix({\n",
    "            \"Loss\": f\"{loss.item():.4f}\",\n",
    "            \"PSNR\": f\"{psnr_value:.2f}\",\n",
    "            \"SSIM\": f\"{ssim_value:.4f}\"\n",
    "        })\n",
    "    print(f'PSNR: {np.mean(psnr_l):.2f} +- {np.std(psnr_l):.2f}, SSIM: {np.mean(ssim_l):.4f} +- {np.std(ssim_l):.4f}')\n",
    "    return loss, np.mean(psnr_l), np.mean(ssim_l)"
   ],
   "id": "bdba813ffc1f130c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_model(args):\n",
    "    print(f\"reconstructing : {args.model}\")\n",
    "    model = Subsampling_Model(\n",
    "        in_chans=1,\n",
    "        out_chans=1,\n",
    "        chans=args.num_chans,\n",
    "        num_pool_layers=args.num_pools,\n",
    "        drop_prob=args.drop_prob,\n",
    "        decimation_rate=args.decimation_rate,\n",
    "        res=args.resolution,\n",
    "        trajectory_learning=args.trajectory_learning,\n",
    "        initialization=args.initialization,\n",
    "        SNR=args.SNR,\n",
    "        n_shots=args.n_shots,\n",
    "        interp_gap=args.interp_gap,\n",
    "        type=args.model\n",
    "    ).to(args.device)\n",
    "    return model\n",
    "\n",
    "def load_model(checkpoint_file):\n",
    "    print(checkpoint_file)\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    args = checkpoint['args']\n",
    "    model = build_model(args)\n",
    "    if args.data_parallel:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer = build_optim(args, model)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return checkpoint, model, optimizer, args\n",
    "\n",
    "def build_optim(args, model):\n",
    "    optimizer = torch.optim.Adam([{'params': model.module.subsampling.parameters(), 'lr': args.sub_lr},\n",
    "                                  {'params': model.module.reconstruction_model.parameters()}], args.lr)\n",
    "    return optimizer"
   ],
   "id": "5cb184521e4fa7c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "unet_t_norm = (Unet_with_trajectory_learning.module.subsampling.x - Unet_without_trajectory_learning.module.subsampling.x).norm()",
   "id": "c288484ff40aad43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "humus_t_norm = (Humus_with_trajectory_learning.module.subsampling.x - Humus_without_trajectory_learning.module.subsampling.x).norm()",
   "id": "d0aefab9bd3ecfb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "checkpoint_files = {\n",
    "    \"Unet_with_trajectory_learning\": \"summary/16/spiral_high_1e-05_0.01_0.01/best_model.pt\",\n",
    "    \"Unet_without_trajectory_learning\": \"summary/16/spiral_high_0_0.01_0.01/best_model.pt\",\n",
    "    \"Humus_with_trajectory_learning\": \"summary/16/spiral_high_1e-05_0.01_0.01_humus/best_model.pt\",\n",
    "    \"Humus_without_trajectory_learning\": \"summary/16/spiral_high_0_0.01_0.01_humus/best_model.pt\",\n",
    "}\n",
    "_, Unet_with_trajectory_learning, _, args = load_model(checkpoint_files[\"Unet_with_trajectory_learning\"])\n",
    "_, Unet_without_trajectory_learning, _, _ = load_model(checkpoint_files[\"Unet_without_trajectory_learning\"])\n",
    "_, Humus_with_trajectory_learning, _, _ = load_model(checkpoint_files[\"Humus_with_trajectory_learning\"])\n",
    "_, Humus_without_trajectory_learning, _, _ = load_model(checkpoint_files[\"Humus_without_trajectory_learning\"])"
   ],
   "id": "ded3f833fd983acf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from models.subsampling_model import Subsampling_Layer\n",
    "subsampling = Subsampling_Layer(10, 1, 0, \"radial\", 16, 20, False).to(\"cpu\")\n",
    "humusBlock = HUMUSBlock(use_checkpoint=False, num_cascades=4, img_size=[320, 320], window_size=4,\n",
    "                                mask_center=False, num_adj_slices=1, in_chans=2).to(\"cpu\")"
   ],
   "id": "cc1d1a15d7f3f02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "loader = create_test_loader(args)\n",
    "for data in loader:\n",
    "    input, target, mean, std, norm = data\n",
    "    s = subsampling(input.unsqueeze(1))\n",
    "    break"
   ],
   "id": "59372eff58468db5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Unet_with_trajectory_learning.to(\"cpu\")\n",
    "del Unet_with_trajectory_learning\n",
    "\n",
    "Unet_without_trajectory_learning.to(\"cpu\")\n",
    "del Unet_without_trajectory_learning\n",
    "\n",
    "Humus_with_trajectory_learning.to(\"cpu\")\n",
    "del Humus_with_trajectory_learning\n",
    "\n",
    "Humus_without_trajectory_learning.to(\"cpu\")\n",
    "del Humus_without_trajectory_learning\n",
    "\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "7275437527d9e17b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"the diff in trajectory for the unet: {unet_t_norm}\")\n",
    "print(f\"the diff in trajectory for the humus: {humus_t_norm}\")"
   ],
   "id": "61ef6ff2c006f5ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "\n",
    "def apply_noise(noise_type = \"random\"):\n",
    "    noise_levels = np.logspace(0, 15, 16)\n",
    "    for model_name, file in checkpoint_files.items():\n",
    "        _, model, _, args = load_model(file)\n",
    "        args.sample_rate = 0.2\n",
    "        test_loader = create_test_loader(args)\n",
    "        initial_trajectory = model.module.subsampling.x.clone()\n",
    "        results = []\n",
    "\n",
    "        for noise_level in noise_levels:\n",
    "            l1_noise = torch.randn_like(initial_trajectory) if noise_type == \"random\" else torch.ones_like(initial_trajectory)\n",
    "            l1_norm = torch.norm(l1_noise, p=1)\n",
    "            l1_noise_scaled = noise_level * l1_noise / l1_norm \n",
    "\n",
    "            l2_noise = torch.randn_like(initial_trajectory) if noise_type == \"random\" else torch.ones_like(initial_trajectory)\n",
    "            l2_norm = torch.norm(l2_noise)\n",
    "            l2_noise_scaled = noise_level * l2_noise / l2_norm \n",
    "\n",
    "            linf_noise = torch.randn_like(initial_trajectory) if noise_type == \"random\" else torch.ones_like(initial_trajectory)\n",
    "            linf_norm = torch.norm(linf_noise, p=float('inf')) \n",
    "            linf_noise_scaled = noise_level * linf_noise / linf_norm  \n",
    "\n",
    "            noisy_trajectory_l1 = initial_trajectory + l1_noise_scaled\n",
    "            noisy_trajectory_l1 = torch.clamp(noisy_trajectory_l1, min=-160, max=160)\n",
    "\n",
    "            noisy_trajectory_l2 = initial_trajectory + l2_noise_scaled\n",
    "            noisy_trajectory_l2 = torch.clamp(noisy_trajectory_l2, min=-160, max=160)\n",
    "\n",
    "            noisy_trajectory_linf = initial_trajectory + linf_noise_scaled\n",
    "            noisy_trajectory_linf = torch.clamp(noisy_trajectory_linf, min=-160, max=160)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.module.subsampling.x.data = noisy_trajectory_l1\n",
    "            dev_loss_l1, noisy_psnr_l1, noisy_ssim_l1 = evaluate(args, model, test_loader)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.module.subsampling.x.data = noisy_trajectory_l2\n",
    "            dev_loss_l2, noisy_psnr_l2, noisy_ssim_l2 = evaluate(args, model, test_loader)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.module.subsampling.x.data = noisy_trajectory_linf\n",
    "            dev_loss_linf, noisy_psnr_linf, noisy_ssim_linf = evaluate(args, model, test_loader)\n",
    "\n",
    "            results.append({\n",
    "                'noise_level': noise_level,\n",
    "                'psnr_l1': noisy_psnr_l1,\n",
    "                'ssim_l1': noisy_ssim_l1,\n",
    "                'psnr_l2': noisy_psnr_l2,\n",
    "                'ssim_l2': noisy_ssim_l2,\n",
    "                'psnr_linf': noisy_psnr_linf,\n",
    "                'ssim_linf': noisy_ssim_linf\n",
    "            })\n",
    "\n",
    "\n",
    "        with open(f'results_{model_name}_{noise_type}.json', 'w') as f:\n",
    "            json.dump(results, f, indent=4)"
   ],
   "id": "c3c1c3e29fbecc8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filenames = {\n",
    "    \"Humus + Trajectory\": \"results_Humus_with_trajectory_learning.json\",\n",
    "    \"Humus - Trajectory\": \"results_Humus_without_trajectory_learning.json\",\n",
    "    \"UNet + Trajectory\":  \"results_Unet_with_trajectory_learning.json\",\n",
    "    \"UNet - Trajectory\":  \"results_Unet_without_trajectory_learning.json\",\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for label, fname in filenames.items():\n",
    "    with open(fname, 'r') as f:\n",
    "        data = json.load(f)  \n",
    "        # If 'data' is a list of dicts, do something like:\n",
    "        #  [ { \"noise_level\": ..., \"psnr_l1\": ..., ... },\n",
    "        #    { \"noise_level\": ..., \"psnr_l1\": ..., ... }, ...]\n",
    "\n",
    "    # Sort by noise_level (if that’s what you want):\n",
    "    data = sorted(data, key=lambda d: d[\"noise_level\"])\n",
    "\n",
    "    noise_levels = []\n",
    "    psnr_l1 = []\n",
    "    ssim_l1 = []\n",
    "    psnr_l2 = []\n",
    "    ssim_l2 = []\n",
    "    psnr_linf = []\n",
    "    ssim_linf = []\n",
    "    \n",
    "    for item in data:\n",
    "        noise_levels.append(item[\"noise_level\"])\n",
    "        psnr_l1.append(item[\"psnr_l1\"])\n",
    "        ssim_l1.append(item[\"ssim_l1\"])\n",
    "        psnr_l2.append(item[\"psnr_l2\"])\n",
    "        ssim_l2.append(item[\"ssim_l2\"])\n",
    "        psnr_linf.append(item[\"psnr_linf\"])\n",
    "        ssim_linf.append(item[\"ssim_linf\"])\n",
    "    \n",
    "    results[label] = {\n",
    "        \"noise_levels\": noise_levels,\n",
    "        \"psnr_l1\": psnr_l1,\n",
    "        \"psnr_l2\": psnr_l2,\n",
    "        \"psnr_linf\": psnr_linf,\n",
    "        \"ssim_l1\": ssim_l1,\n",
    "        \"ssim_l2\": ssim_l2,\n",
    "        \"ssim_linf\": ssim_linf\n",
    "    }\n",
    "\n",
    "# ----------------------------\n",
    "# Now plot the aggregated data\n",
    "# ----------------------------\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 6), sharex=True)\n",
    "axs = axs.ravel()\n",
    "\n",
    "metric_labels = [\"psnr_l1\", \"psnr_l2\", \"psnr_linf\", \"ssim_l1\", \"ssim_l2\", \"ssim_linf\"]\n",
    "titles       = [\"PSNR (L1)\", \"PSNR (L2)\", \"PSNR (L∞)\", \"SSIM (L1)\", \"SSIM (L2)\", \"SSIM (L∞)\"]\n",
    "\n",
    "for ax, metric_label, title in zip(axs, metric_labels, titles):\n",
    "    for label in results:\n",
    "        noise_levels = results[label][\"noise_levels\"]\n",
    "        metric_vals  = results[label][metric_label]\n",
    "        ax.plot(noise_levels, metric_vals, marker='o', label=label)\n",
    "        \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Noise level\")\n",
    "    ax.set_ylabel(metric_label.upper())\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.grid(True)\n",
    "    ax.legend(fontsize=8)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "3785d96e2075294f"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
