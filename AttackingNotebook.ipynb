{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b9019c6b00607e",
   "metadata": {},
   "source": [
    "This Notebook is dedicated for the research on MRI Domain adaptation project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c5176cf-9f0f-4f9f-a5b5-47c98403246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33695d52-bd89-4ea3-9514-b9d2fecf1a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "print(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac355fe450fc2433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from common.args import Args\n",
    "from data import transforms2\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7bb0805-6f7e-45ad-8bdb-ad94b2e713fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammedw/miniconda3/envs/mpilot-py310/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mohammedw/miniconda3/envs/mpilot-py310/lib/python3.12/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/mohammedw/miniconda3/envs/mpilot-py310/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from models.subsampling_model import Subsampling_Model\n",
    "from scipy.spatial import distance_matrix\n",
    "import scipy.io as sio\n",
    "from common.utils import get_vel_acc\n",
    "#from common.evaluate import psnr, ssim\n",
    "from fastmri.losses import SSIMLoss\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92607383d2ce5b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import fastmri.models\n",
    "from models.rec_models.unet_model import UnetModel\n",
    "from models.rec_models.complex_unet import ComplexUnetModel\n",
    "import data.transforms2 as transforms\n",
    "from pytorch_nufft.nufft2 import nufft, nufft_adjoint\n",
    "import numpy as np\n",
    "from WaveformProjection.run_projection import proj_handler\n",
    "import matplotlib.pylab as P\n",
    "from models.rec_models.vision_transformer import VisionTransformer\n",
    "from models.rec_models.recon_net import ReconNet\n",
    "from models.rec_models.humus_net import HUMUSNet, HUMUSBlock\n",
    "from  models.VarBlock import VarNet\n",
    "from typing import Tuple\n",
    "from fastmri.data.subsample import create_mask_for_mask_type\n",
    "from fastmri.data.transforms import apply_mask\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9955b00d-40b2-45e5-abfa-cf291905ce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "print(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d5d607d-8936-4541-b12d-6a70cd573d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def pgd_attack_with_trajectory(model, input_tensor, target_tensor, epsilon, alpha=0.1, steps=10, norm='linf'):\n",
    "    \"\"\"\n",
    "    PGD attack with support for L1, L2, and Linf norms\n",
    "    Args:\n",
    "        model: Model to attack\n",
    "        input_tensor: Clean input tensor\n",
    "        target_tensor: Ground truth target\n",
    "        epsilon: Attack strength\n",
    "        alpha: Attack step size\n",
    "        steps: Number of PGD steps\n",
    "        norm: Norm to use for projection ('l1', 'l2', or 'linf')\n",
    "    \"\"\"\n",
    "    device = torch.device(f'cuda:{model.device_ids[0]}')\n",
    "    model = model.to(device)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    target_tensor = target_tensor.to(device)\n",
    "    \n",
    "    alpha = epsilon/steps\n",
    "    # Initialize perturbation\n",
    "    #print(\"Initial requires_grad:\", model.module.subsampling.x.requires_grad)  # Output: False\n",
    "    original_trajectory = model.module.subsampling.x.detach().clone()\n",
    "    perturbation = torch.zeros_like(original_trajectory, requires_grad=True).to(device)\n",
    "    model.module.subsampling.x.requires_grad_(True)\n",
    "    #print(\"Updated requires_grad:\", model.module.subsampling.x.requires_grad)  # Output: True\n",
    "    \n",
    "    lowest_psnr = float('inf')\n",
    "    best_perturbation = torch.zeros_like(perturbation)\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        # Apply perturbation\n",
    "        perturbed_trajectory = original_trajectory + perturbation\n",
    "        perturbed_trajectory = torch.clamp(perturbed_trajectory, min=-160, max=160)\n",
    "        \n",
    "        # Zero gradients\n",
    "        if model.module.subsampling.x.grad is not None:\n",
    "            model.module.subsampling.x.grad.zero_()\n",
    "        \n",
    "        # Forward pass\n",
    "        model.module.subsampling.x.data = perturbed_trajectory\n",
    "        output = model(input_tensor.unsqueeze(1))\n",
    "        target = target_tensor.view_as(output) if output.shape != target_tensor.shape else target_tensor\n",
    "\n",
    "        # Calculate loss and PSNR\n",
    "        loss = F.l1_loss(output.to(device), target.to(device))\n",
    "        current_psnr = psnr(target.detach().cpu().numpy(), \n",
    "                          output.detach().cpu().numpy())\n",
    "\n",
    "        # Track best attack\n",
    "        if current_psnr < lowest_psnr:\n",
    "            lowest_psnr = current_psnr\n",
    "            best_perturbation = perturbation.detach().clone()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update perturbation based on norm\n",
    "        if norm == 'linf':\n",
    "            # Linf: sign of gradient\n",
    "            if model.module.subsampling.x.grad != None:\n",
    "                perturbation.data = perturbation.data + alpha * model.module.subsampling.x.grad.sign()\n",
    "            else:\n",
    "                 perturbation.data = perturbation.data\n",
    "            perturbation.data = torch.clamp(perturbation.data, -epsilon, epsilon)\n",
    "        elif norm == 'l2':\n",
    "            # L2: normalized gradient\n",
    "            grad = model.module.subsampling.x.grad\n",
    "            l2_norm = torch.norm(grad.view(grad.shape[0], -1), p=2, dim=1)\n",
    "            normalized_grad = grad / (l2_norm.view(-1, 1, 1) + 1e-10)\n",
    "            perturbation.data = perturbation.data + alpha * normalized_grad\n",
    "            perturbation.data = project_l2(perturbation.data, epsilon)\n",
    "        elif norm == 'l1':\n",
    "            # L1: coordinate-wise update\n",
    "            grad = model.module.subsampling.x.grad\n",
    "            perturbation.data = perturbation.data + alpha * grad.sign()\n",
    "            perturbation.data = project_l1(perturbation.data, epsilon)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported norm: {norm}\")\n",
    "    \n",
    "    # Restore original trajectory\n",
    "    model.module.subsampling.x.data = original_trajectory.data\n",
    "    return best_perturbation, lowest_psnr\n",
    "\n",
    "# Projection functions (same as provided)\n",
    "def project_linf(perturbation, epsilon):\n",
    "    return torch.clamp(perturbation, -epsilon, epsilon)\n",
    "\n",
    "def project_l2(perturbation, epsilon):\n",
    "    flat = perturbation.view(perturbation.shape[0], -1)\n",
    "    norm = flat.norm(p=2, dim=1, keepdim=True)\n",
    "    factor = torch.min(torch.ones_like(norm), epsilon / (norm + 1e-10))\n",
    "    projected = flat * factor\n",
    "    return projected.view_as(perturbation)\n",
    "\n",
    "def project_l1(perturbation, epsilon):\n",
    "    original_shape = perturbation.shape\n",
    "    x_flat = perturbation.view(perturbation.shape[0], -1)\n",
    "    abs_x = torch.abs(x_flat)\n",
    "\n",
    "    sorted_x, _ = torch.sort(abs_x, descending=True, dim=1)\n",
    "    cumsum = torch.cumsum(sorted_x, dim=1)\n",
    "\n",
    "    rho = (sorted_x * torch.arange(1, x_flat.shape[1] + 1, device=x_flat.device)) > (cumsum - epsilon)\n",
    "    rho_idx = rho.sum(dim=1) - 1\n",
    "    theta = (cumsum.gather(1, rho_idx.unsqueeze(1)) - epsilon) / (rho_idx + 1).float().unsqueeze(1)\n",
    "    theta = torch.clamp(theta, min=0)\n",
    "\n",
    "    projected = torch.sign(x_flat) * torch.clamp(abs_x - theta, min=0)\n",
    "    return projected.view(original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b2a9167-f5cf-4b91-b847-3eb0bb87d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attack_on_models(model_dict, input_img, target_img, epsilon=3):\n",
    "    \"\"\"\n",
    "    For each model in the dictionary, show prediction before and after PGD attack on the trajectory.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_dict: dict of {name: model}\n",
    "    - input_img: tensor, shape [1, H, W] or [H, W] (will be unsqueezed)\n",
    "    - target_img: ground truth (used for attack)\n",
    "    - epsilon: perturbation bound for PGD attack\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    for name, model in model_dict.items():\n",
    "        print(f\"--- {name} ---\")\n",
    "        \n",
    "        # Show prediction before attack\n",
    "        print(\"Clean prediction:\")\n",
    "        clean_image = model(input_img.unsqueeze(1))\n",
    "        show_image(clean_image[0])\n",
    "        \n",
    "        # Apply PGD attack on trajectory\n",
    "        delta_traj = pgd_attack_with_trajectory(model, input_img, target_img, epsilon)[0]\n",
    "        model.module.get_trajectory().data += delta_traj\n",
    "        \n",
    "        # Show prediction after attack\n",
    "        print(\"After PGD attack on trajectory:\")\n",
    "        attacked_image = model(input_img.unsqueeze(1))\n",
    "        show_image(attacked_image[0])\n",
    "        \n",
    "        # Optionally, plot both side by side\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        axs[0].imshow(clean_image.detach().cpu().squeeze(), cmap='gray')\n",
    "        axs[0].set_title(f\"{name} - Clean\")\n",
    "        axs[0].axis('off')\n",
    "        \n",
    "        axs[1].imshow(attacked_image.detach().cpu().squeeze(), cmap='gray')\n",
    "        axs[1].set_title(f\"{name} - After Attack\")\n",
    "        axs[1].axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "468c27aa-f88f-421b-87f9-d50c44f29f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_trajectory_noise_effects(model_dict, input_img, std_list=[25, 50, 75, 100], clip_range=(-160, 160)):\n",
    "    \"\"\"\n",
    "    For each model, shows predictions with clean and noisy trajectories.\n",
    "\n",
    "    Parameters:\n",
    "    - model_dict: dict of {name: model}\n",
    "    - input_img: torch.Tensor, shape [1, H, W]\n",
    "    - std_list: list of std values for Gaussian noise\n",
    "    - clip_range: tuple (min, max) for trajectory clipping\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    for name, model in model_dict.items():\n",
    "        print(f\"=== {name} ===\")\n",
    "        model.eval()\n",
    "\n",
    "        # Get base trajectory\n",
    "        base_traj = model.module.get_trajectory().data.clone().cpu().numpy()\n",
    "\n",
    "        # Store all reconstructions: (title, image)\n",
    "        reconstructions = []\n",
    "\n",
    "        # 1. Clean prediction\n",
    "        model.module.get_trajectory().data = torch.tensor(base_traj, dtype=torch.float32).to(model.module.get_trajectory().device)\n",
    "        clean_pred = model(input_img.unsqueeze(1)).detach().cpu()\n",
    "        print(f\"Clean prediction shape: {clean_pred.shape}\")\n",
    "\n",
    "        # Ensure 2D shape [H, W]\n",
    "        clean_pred = torch.squeeze(clean_pred)  # Remove all singleton dimensions\n",
    "        if clean_pred.ndim == 3:\n",
    "            clean_pred = clean_pred[0]  # Select first channel if necessary\n",
    "        elif clean_pred.ndim != 2:\n",
    "            raise ValueError(f\"Unexpected clean_pred shape: {clean_pred.shape}. Expected 2D array.\")\n",
    "\n",
    "        reconstructions.append((\"Clean\", clean_pred))\n",
    "\n",
    "        # 2-5. Predictions with noisy trajectories\n",
    "        for std in std_list:\n",
    "            noise = np.random.normal(loc=0.0, scale=std, size=base_traj.shape)\n",
    "            noisy_traj = np.clip(base_traj + noise, clip_range[0], clip_range[1]).astype(np.float32)\n",
    "\n",
    "            model.module.get_trajectory().data = torch.tensor(noisy_traj, dtype=torch.float32).to(model.module.get_trajectory().device)\n",
    "            noisy_pred = model(input_img.unsqueeze(1)).detach().cpu()\n",
    "            print(f\"Noisy prediction shape (std={std}): {noisy_pred.shape}\")\n",
    "\n",
    "            # Ensure 2D shape [H, W]\n",
    "            noisy_pred = torch.squeeze(noisy_pred)  # Remove all singleton dimensions\n",
    "            if noisy_pred.ndim == 3:\n",
    "                noisy_pred = noisy_pred[0]  # Select first channel if necessary\n",
    "            elif noisy_pred.ndim != 2:\n",
    "                raise ValueError(f\"Unexpected noisy_pred shape: {noisy_pred.shape}. Expected 2D array.\")\n",
    "\n",
    "            reconstructions.append((f\"STD={std}\", noisy_pred))\n",
    "\n",
    "        # Plotting\n",
    "        fig, axs = plt.subplots(1, len(reconstructions), figsize=(4 * len(reconstructions), 4))\n",
    "        if len(reconstructions) == 1:\n",
    "            axs = [axs]  # Make iterable if only one subplot\n",
    "\n",
    "        for ax, (title, img) in zip(axs, reconstructions):\n",
    "            ax.imshow(img.numpy(), cmap='gray')\n",
    "            ax.set_title(title)\n",
    "            ax.axis('off')\n",
    "        fig.suptitle(f\"Model: {name}\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de98db2a-eeca-403c-b474-a2ffcf198c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(args, model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "\n",
    "    psnr_l = []\n",
    "    ssim_l = []\n",
    "    start_epoch = start_iter = time.perf_counter()\n",
    "    print(f'a_max={args.a_max}, v_max={args.v_max}')\n",
    "\n",
    "    for iter, data in enumerate(data_loader):\n",
    "        torch.cuda.empty_cache()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input, target, mean, std, norm = data\n",
    "        if input is None:\n",
    "            print(\"skipping\")\n",
    "            continue\n",
    "\n",
    "        noise = torch.zeros_like(input)\n",
    "        if args.noise_behaviour == \"image\" and random.random() <= (1 - args.noise_p):\n",
    "            print(\"applied!\")\n",
    "            noise = torch.randn_like(input) * args.std\n",
    "\n",
    "        input = input + noise\n",
    "        input = input.to(args.device)\n",
    "        target = target.to(args.device)\n",
    "\n",
    "\n",
    "        output = model(input.unsqueeze(1))\n",
    "        x = model.module.get_trajectory()\n",
    "        v, a = get_vel_acc(x)\n",
    "        acc_loss = torch.sqrt(torch.sum(torch.pow(F.softshrink(a, args.a_max).abs() + 1e-8, 2)))\n",
    "        vel_loss = torch.sqrt(torch.sum(torch.pow(F.softshrink(v, args.v_max).abs() + 1e-8, 2)))\n",
    "        resolution = target.shape[-1]\n",
    "\n",
    "        data_min = target.min()\n",
    "        data_max = target.max()\n",
    "        target_normalized = (target - data_min) / (data_max - data_min)\n",
    "        output_normalized = (output - data_min) / (data_max - data_min)\n",
    "\n",
    "        loss_l1 = F.l1_loss(output, target)\n",
    "        psnr_l.append(psnr(target.detach().cpu().numpy(), output.detach().cpu().numpy()))\n",
    "        ssim_l.append(ssim(target.detach().cpu().numpy(), output.detach().cpu().numpy()))\n",
    "        rec_loss = loss_l1 #+ dcLoss # SSIMLoss().to(args.device)(output, target, data_range) # F.l1_loss(output, target)\n",
    "        if args.TSP and epoch < args.TSP_epoch:\n",
    "            loss = args.rec_weight * rec_loss\n",
    "        else:\n",
    "            loss = args.rec_weight * rec_loss + args.vel_weight * vel_loss + args.acc_weight * acc_loss\n",
    "\n",
    "        #if vel_loss + acc_loss > 1e-3:\n",
    "        #    optimize_trajectory(args, model)\n",
    "\n",
    "        #print(\"before backprop:\", model.module.subsampling.x)\n",
    "        loss.backward()\n",
    "        #print(\"after backprop:\", model.module.subsampling.x)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1, norm_type=1.)\n",
    "        optimizer.step()\n",
    "        model.module.subsampling.attack_trajectory = None\n",
    "\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "        # writer.add_scalar('TrainLoss', loss.item(), global_step + iter)\n",
    "\n",
    "        if iter % args.report_interval == 0:\n",
    "            logging.info(\n",
    "                f'Iter = [{iter:4d}/{len(data_loader):4d}] '\n",
    "                f'Loss = {loss.item():.4g} Avg Loss = {avg_loss:.4g} '\n",
    "                f'rec_loss: {rec_loss:.4g}, vel_loss: {vel_loss:.4g}, acc_loss: {acc_loss:.4g}'\n",
    "                f'PSNR: {np.mean(psnr_l):.2f} +- {np.std(psnr_l):.2f}, SSIM: {np.mean(ssim_l):.4f} +- {np.std(ssim_l):.4f}'\n",
    "            )\n",
    "        start_iter = time.perf_counter()\n",
    "    return avg_loss, time.perf_counter() - start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de30c0d7-4fe9-4e0b-aed2-cc09980b840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torchvision.transforms import Resize, ToPILImage\n",
    "\n",
    "def show_image(source):\n",
    "    source = source.clone()\n",
    "    source.reshape(320,320)\n",
    "    image = source\n",
    "    image -= image.min()\n",
    "    max_val = image.max()\n",
    "    if max_val > 0:\n",
    "        image /= max_val\n",
    "    source = image\n",
    "    grid = torchvision.utils.make_grid(source, nrow=4, pad_value=1)\n",
    "    numpy_image = grid.permute(1, 2, 0).cpu().detach().numpy()\n",
    "\n",
    "    # Save or display the image\n",
    "    plt.imshow(numpy_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80837a64-3dfe-4af3-88f6-4d21538f32ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def show_trajectory_with_noise(model, std_list=[100]):\n",
    "    \"\"\"\n",
    "    Displays the original trajectory from the model and additional trajectories\n",
    "    with Gaussian noise added using different standard deviations.\n",
    "    \n",
    "    Args:\n",
    "        model: A PyTorch model with `module.get_trajectory()` method.\n",
    "        std_list: A list of standard deviation values to use for Gaussian noise.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Extract base trajectory\n",
    "    base_traj = model.module.get_trajectory().detach().cpu().numpy()\n",
    "\n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # Plot base trajectory\n",
    "    #plot_trajectory(base_traj)\n",
    "\n",
    "    # Plot noisy trajectories\n",
    "    for std in std_list:\n",
    "        noise = np.random.normal(loc=0.0, scale=std, size=base_traj.shape)\n",
    "        noisy_traj = base_traj + noise\n",
    "        noisy_traj = np.clip(noisy_traj, -160, 160)\n",
    "        plot_trajectory(noisy_traj)\n",
    "        plt.show()\n",
    "\n",
    "    plt.title(\"Trajectories with Different Gaussian Noise Levels\")\n",
    "    plt.xlabel(\"kx\")\n",
    "    plt.ylabel(\"ky\")\n",
    "    plt.axis('equal')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a962ce7-7714-4ce4-96d6-548f3c635096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory(x):\n",
    "    fig = plt.figure(figsize=[10, 10])\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.axis([-165, 165, -165, 165])\n",
    "    for i in range(x.shape[0]):\n",
    "        ax.plot(x[i, :, 0], x[i, :, 1])\n",
    "    return fig\n",
    "\n",
    "def show_trajectory(model):\n",
    "    %matplotlib inline\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    x = model.module.get_trajectory()\n",
    "    fig = plot_trajectory(x.detach().cpu().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72201377-f5af-4526-9f2d-d6222074720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_batch(dataloader):\n",
    "    \"\"\"\n",
    "    Get the first example from the DataLoader without iteration.\n",
    "\n",
    "    Args:\n",
    "    - dataloader: PyTorch DataLoader object\n",
    "\n",
    "    Returns:\n",
    "    - first_input: The first input example in the dataset.\n",
    "    - first_label: The label for the first input example.\n",
    "    \"\"\"\n",
    "    inputs, labels, _, _, _ = next(iter(dataloader))\n",
    "    first_input = inputs\n",
    "    first_target = labels\n",
    "    \n",
    "    return first_input, first_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb2481c222ce61c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Device Name: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA device not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1584882-28ed-4284-8c9f-df01536501e1",
   "metadata": {},
   "source": [
    "The `%matplotlib inline` option is needed to plot graphs in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1b49a25-95b7-45e0-a243-03d68ff56701",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e936f96a128718a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "620e0610-9c88-4198-9cc3-530aa9f10c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright (c) Facebook, Inc. and its affiliates.\n",
    "\n",
    "This source code is licensed under the MIT license found in the\n",
    "LICENSE file in the root directory of this source tree.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class TrajectoryFunc:\n",
    "    def __init__(self, center_fractions, accelerations):\n",
    "        if len(center_fractions) != len(accelerations):\n",
    "            raise ValueError('Number of center fractions should match number of accelerations')\n",
    "\n",
    "        self.center_fractions = center_fractions\n",
    "        self.accelerations = accelerations\n",
    "        self.rng = np.random.RandomState()\n",
    "        \n",
    "        \n",
    "    def mask_to_trajectory(self, mask):\n",
    "        mask = np.squeeze(mask)\n",
    "\n",
    "        x_values = [x for x in range(-160, 160)]\n",
    "\n",
    "        trajectory = []\n",
    "        for i in range(mask.shape[0]):\n",
    "            #print(mask[i].item())\n",
    "            if mask[i].item() == 1:\n",
    "                for x in x_values:\n",
    "                    trajectory.append([x, i - 160])\n",
    "        return trajectory\n",
    "\n",
    "\n",
    "    def __call__(self, shape, seed=None):\n",
    "        if len(shape) < 3:\n",
    "            raise ValueError('Shape should have 3 or more dimensions')\n",
    "\n",
    "        self.rng.seed(seed)\n",
    "        num_cols = shape[-2]\n",
    "\n",
    "        choice = self.rng.randint(0, len(self.accelerations))\n",
    "        center_fraction = self.center_fractions[choice]\n",
    "        acceleration = self.accelerations[choice]\n",
    "\n",
    "        # Desired number of total columns to retain\n",
    "        num_low_freqs = int(round(num_cols * center_fraction))\n",
    "        num_total_samples = int(round(num_cols / acceleration))\n",
    "        num_high_freqs = num_total_samples - num_low_freqs\n",
    "        \n",
    "        # Generate mask with zeros\n",
    "        mask = np.zeros(num_cols, dtype=np.float32)\n",
    "\n",
    "        # Always keep low-frequency columns in the center\n",
    "        pad = (num_cols - num_low_freqs + 1) // 2\n",
    "        mask[pad:pad + num_low_freqs] = 1\n",
    "\n",
    "        # Randomly choose high-frequency indices outside the low-freq region\n",
    "        low_freq_start = pad\n",
    "        low_freq_end = pad + num_low_freqs\n",
    "        available_indices = list(range(0, low_freq_start)) + list(range(low_freq_end, num_cols))\n",
    "        \n",
    "        if num_high_freqs > len(available_indices):\n",
    "            raise ValueError(\"Not enough columns to sample from.\")\n",
    "\n",
    "        high_freq_indices = self.rng.choice(available_indices, size=num_high_freqs, replace=False)\n",
    "        mask[high_freq_indices] = 1\n",
    "\n",
    "        # Reshape mask\n",
    "        mask_shape = [1 for _ in shape]\n",
    "        mask_shape[-2] = num_cols\n",
    "        mask = torch.from_numpy(mask.reshape(*mask_shape))\n",
    "        return torch.tensor(self.mask_to_trajectory(mask)).reshape(int(round(num_cols / acceleration)),-1,2)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee04cf76-05f9-43e8-9f31-3abb9934d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectoryFunc = TrajectoryFunc([0.0325], [10])\n",
    "trajectory = trajectoryFunc([32, 320, 320])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edb5b544-11bb-4e6a-87f1-675d746aeec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 320, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2311aecb-729a-49b7-8744-25257c2cd6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e33e98-68e2-47cd-b8a1-26cc2fc55549",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SliceData\n",
    "* Responsible of defining how we will read the data from the .h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e3ddd59-110e-4ac3-99d5-0ef398295c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import h5py\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import os\n",
    "import torch\n",
    "\n",
    "class SliceData(Dataset):\n",
    "    def __init__(self, root, transform, challenge=\"multicoil\", sample_rate=1, resolution=(320, 320)):\n",
    "        assert challenge in [\"singlecoil\", \"multicoil\"], \"Challenge must be 'singlecoil' or 'multicoil'\"\n",
    "        self.transform = transform\n",
    "        self.challenge = challenge\n",
    "        self.examples = []\n",
    "\n",
    "        files = list(pathlib.Path(root).iterdir())\n",
    "        if sample_rate < 1:\n",
    "            random.shuffle(files)\n",
    "            num_files = round(len(files) * sample_rate)\n",
    "            files = files[:num_files]\n",
    "\n",
    "        for fname in sorted(files):\n",
    "            try:\n",
    "                with h5py.File(fname, 'r') as data:\n",
    "                    if \"kspace\" not in data:\n",
    "                        continue  # Skip files without k-space data\n",
    "                    \n",
    "                    kspace = data[\"kspace\"]\n",
    "                    if self.challenge == \"multicoil\":\n",
    "                        if len(kspace.shape) != 4:  # Ensure it follows (num_slices, num_coils, height, width)\n",
    "                            continue\n",
    "                    else:\n",
    "                        if len(kspace.shape) != 3:  # Ensure it follows (num_slices, height, width)\n",
    "                            continue\n",
    "                    \n",
    "                    num_slices = kspace.shape[0]\n",
    "                    temp = [(fname, slice) for slice in range(5, num_slices - 2)]\n",
    "                    for (fname, slice) in temp: \n",
    "                        try:\n",
    "                            with h5py.File(fname, 'r') as data:\n",
    "                                kspace = data[\"kspace\"][slice]\n",
    "                                if \"reconstruction_rss\" in data:\n",
    "                                    try:\n",
    "                                        target = data[\"reconstruction_rss\"][slice]\n",
    "                                        self.transform(kspace, target, data.attrs, fname.name, slice, self.challenge)\n",
    "                                        self.examples.append((fname, slice))\n",
    "                                    except Exception as e:\n",
    "                                        print(e)\n",
    "                                        continue\n",
    "                                else:\n",
    "                                    continue\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            continue\n",
    "                    print(len(self.examples))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        fname, slice = self.examples[i]\n",
    "        try:\n",
    "            with h5py.File(fname, 'r') as data:\n",
    "                kspace = data[\"kspace\"][slice]                \n",
    "                if \"reconstruction_rss\" in data:\n",
    "                    try:\n",
    "                        target = data[\"reconstruction_rss\"][slice]\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        return None  \n",
    "                else:\n",
    "                    target = None\n",
    "                return self.transform(kspace, target, data.attrs, fname.name, slice, self.challenge)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab66bf5-0581-44c7-812b-650b2d89b13a",
   "metadata": {},
   "source": [
    "### DataTransform\n",
    "Resposible for preparing the data to be in the shape the model expects\n",
    "\n",
    "### create_YYY_dataset\n",
    "Creates the YYY dataset\n",
    "\n",
    "### create_data_loaders\n",
    "Creates the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7d88be6376f851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchvision import transforms as visonTransforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DataTransform:\n",
    "    def __init__(self, resolution):\n",
    "        self.resolution = resolution\n",
    "        self.resize_transform = visonTransforms.Resize((self.resolution, self.resolution)) \n",
    "\n",
    "    def __call__(self, kspace, target, attrs, fname, slice, challange=\"multicoil\"):\n",
    "        kspace = transforms.to_tensor(kspace)\n",
    "        image = transforms.ifft2_regular(kspace)\n",
    "        # Convert from two channels to complex\n",
    "        if len(image.shape) == 4:\n",
    "            image = image.permute(0, 3, 1, 2)\n",
    "            if image.shape[-2] < self.resolution or image.shape[-1] < self.resolution:\n",
    "                    image = F.interpolate(image, size=(self.resolution, self.resolution), mode='bilinear', align_corners=False)\n",
    "            image = image.permute(0, 2, 3, 1)\n",
    "        \n",
    "        image = transforms.complex_center_crop(image, (self.resolution, self.resolution))\n",
    "        image, mean, std = transforms.normalize_instance(image, eps=1e-11)\n",
    "        \n",
    "        #print(image.shape)\n",
    "        target = transforms.to_tensor(target)\n",
    "        target, mean, std = transforms.normalize_instance(target, eps=1e-11)\n",
    "        mean = std = 0\n",
    "\n",
    "        target = target.unsqueeze(0).unsqueeze(0)\n",
    "        if target.shape[-2] < self.resolution or target.shape[-1] < self.resolution:\n",
    "            target = F.interpolate(target, size=(self.resolution, self.resolution), mode='bilinear', align_corners=False)\n",
    "        target = target.squeeze(0).squeeze(0) \n",
    "            \n",
    "        target = transforms.center_crop(target, (self.resolution, self.resolution))\n",
    "        \n",
    "        return image.mean(0) if challange == \"multicoil\" else image, target, mean, std, target\n",
    "\n",
    "        \n",
    "\n",
    "def create_knee_dataset(args):\n",
    "    dev_data = SliceData(\n",
    "        root=\"/datasets/4VISTA/fastmri_knee/singlecoil_val\",\n",
    "        challenge=\"singlecoil\",\n",
    "        transform=DataTransform(args.resolution),\n",
    "        sample_rate=args.sample_rate)\n",
    "    return dev_data\n",
    "\n",
    "def create_m4raw_dataset(args):\n",
    "    dev_data = SliceData(\n",
    "        root=args.data_path / f'm4raw/multicoil_val',\n",
    "        challenge=\"multicoil\",\n",
    "        transform=DataTransform(args.resolution),\n",
    "        sample_rate=args.sample_rate)\n",
    "    return dev_data\n",
    "\n",
    "def create_datasets(args):\n",
    "    dev_data = SliceData(\n",
    "        root=args.data_path / f'multicoil_val',\n",
    "        challenge=\"multicoil\",\n",
    "        transform=DataTransform(args.resolution),\n",
    "        sample_rate=args.sample_rate)\n",
    "\n",
    "    return dev_data\n",
    "\n",
    "def create_data_loaders(args, knee = False, m4raw = False):\n",
    "    if knee:\n",
    "        dev_data = create_knee_dataset(args)\n",
    "    elif m4raw: \n",
    "        dev_data = create_m4raw_dataset(args)\n",
    "    else:\n",
    "        dev_data = create_datasets(args)\n",
    "    dev_loader = DataLoader(\n",
    "        dataset=dev_data,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=20,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return dev_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "891d8b85-59d4-4700-a9a8-b75d4141c3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch._C._fft.fft_ifft>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.fft.ifft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdba813ffc1f130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(args, epoch, model, data_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    psnr_l = []\n",
    "    ssim_l = []\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        if epoch > 0:\n",
    "            progress_bar = tqdm(enumerate(data_loader), total=len(data_loader), desc=f\"Evaluating Epoch {epoch}\")\n",
    "            \n",
    "            for iter, data in progress_bar:\n",
    "                input, target, mean, std, norm = data\n",
    "                input = input.to(args.device)\n",
    "                resolution = target.shape[-1]\n",
    "                target = target.to(args.device)\n",
    "\n",
    "                output = model(input.unsqueeze(1))\n",
    "                recons = output.to('cpu').squeeze(1).view(target.shape)\n",
    "                recons = recons.squeeze()\n",
    "                if output.shape != target.shape:\n",
    "                    target = target.view_as(output)\n",
    "\n",
    "                loss = F.l1_loss(output, target)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                target = target.view(-1, resolution, resolution)\n",
    "                recons = recons.view(target.shape)\n",
    "\n",
    "                psnr_value = psnr(target.to('cpu').numpy(), recons.numpy())\n",
    "                ssim_value = ssim(target.to('cpu').numpy(), recons.numpy())\n",
    "                psnr_l.append(psnr_value)\n",
    "                ssim_l.append(ssim_value)\n",
    "\n",
    "                # Update tqdm progress bar with current metrics\n",
    "                progress_bar.set_postfix({\n",
    "                    \"Loss\": f\"{loss.item():.4f}\",\n",
    "                    \"PSNR\": f\"{psnr_value:.2f}\",\n",
    "                    \"SSIM\": f\"{ssim_value:.4f}\"\n",
    "                })\n",
    "\n",
    "        print(f'PSNR: {np.mean(psnr_l):.2f}  {np.std(psnr_l):.2f}, SSIM: {np.mean(ssim_l):.4f}  {np.std(ssim_l):.4f}')\n",
    "        \n",
    "        x = model.module.get_trajectory()\n",
    "        v, a = get_vel_acc(x)\n",
    "        acc_loss = torch.sqrt(torch.sum(torch.pow(F.softshrink(a, args.a_max), 2)))\n",
    "        vel_loss = torch.sqrt(torch.sum(torch.pow(F.softshrink(v, args.v_max), 2)))\n",
    "        rec_loss = np.mean(losses)\n",
    "\n",
    "        return np.mean(losses), np.mean(psnr_l), np.mean(ssim_l), np.std(psnr_l), np.std(ssim_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cb184521e4fa7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(args):\n",
    "    print(f\"reconstructing : {args.model}\")\n",
    "    model = Subsampling_Model(\n",
    "        in_chans=args.in_chans,\n",
    "        out_chans=args.out_chans,\n",
    "        chans=args.num_chans,\n",
    "        num_pool_layers=args.num_pools,\n",
    "        drop_prob=args.drop_prob,\n",
    "        decimation_rate=args.decimation_rate,\n",
    "        res=args.resolution,\n",
    "        trajectory_learning=args.trajectory_learning,\n",
    "        initialization=args.initialization,\n",
    "        SNR=args.SNR,\n",
    "        n_shots=args.n_shots,\n",
    "        interp_gap= 1,\n",
    "        type=args.model,\n",
    "        img_size=args.img_size,\n",
    "        window_size=args.window_size,\n",
    "        embed_dim=args.embed_dim,\n",
    "        num_blocks=args.num_blocks,\n",
    "        sample_per_shot=args.sample_per_shot\n",
    "    ).to(args.device)\n",
    "    return model\n",
    "\n",
    "def load_model(checkpoint_file):\n",
    "    print(checkpoint_file)\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    args = checkpoint['args']\n",
    "    args.trajectory_learning = 1\n",
    "    model = build_model(args)\n",
    "    if args.data_parallel:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer = build_optim(args, model)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return checkpoint, model, optimizer, args\n",
    "\n",
    "def build_optim(args, model):\n",
    "    optimizer = torch.optim.Adam([{'params': model.module.subsampling.parameters(), 'lr': args.sub_lr},\n",
    "                                  {'params': model.module.reconstruction_model.parameters()}], args.lr)\n",
    "    return optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73683a60-d51b-4eaf-931d-2909bd7dac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_files = {\n",
    "    \"Unet_with_trajectory_learning\": \"\"\"/home/mohammed-wa/PycharmProjects/MPILOT/summary/16/radial_0.0005_0.075_0.001_0.001_changing_downwards_10_Unet/best_model.pt\"\"\",\n",
    "    \"Unet_with_trajectory_learning_with_PGD_Train_3norm\" : \"/home/mohammed-wa/PycharmProjects/MPILOT/summary/16/radial_0.0005_0.075_0.001_0.001_changing_downwards_10_Unet_random_PGD_noise_steps10_start_epsilon3_noise_typelinf_P0.5/best_model.pt\",\n",
    "    \"Unet_with_trajectory_learning_with_noise_75_Train\" : \"/home/mohammed-wa/PycharmProjects/MPILOT/summary/16/radial_0.0005_0.075_0.001_0.001_changing_downwards_10_Unet_noise_std_75_noise_p_0.5/best_model.pt\",\n",
    "    \"Unet_with_trajectory_learning_with_noise_50_Train\" : \"/home/mohammed-wa/PycharmProjects/MPILOT/summary/16/radial_0.0005_0.075_0.001_0.001_changing_downwards_10_Unet_noise_std_50_noise_p_0.5/best_model.pt\",\n",
    "    \"Unet_with_trajectory_learning_with_noise_25_Train\" : \"/home/mohammed-wa/PycharmProjects/MPILOT/summary/16/radial_0.0005_0.075_0.001_0.001_changing_downwards_10_Unet_noise_std_25_noise_p_0.5/best_model.pt\",\n",
    "    \"Unet_with_trajectory_learning_with_image_1_Train\" : \"/home/mohammed-wa/PycharmProjects/MPILOT/summary/16/radial_0.0005_0.075_0.001_0.001_changing_downwards_10_Unet_image_std_1_noise_p_0.5/best_model.pt\",\n",
    "    \"Unet_with_trajectory_learning_with_image_1_std_75_Train\" : \"/home/mohammed-wa/PycharmProjects/MPILOT/summary/16/radial_0.0005_0.075_0.001_0.001_changing_downwards_10_Unet_noiseImage_std_75_std_image1_noise_p_0.5/best_model.pt\",\n",
    "    \"Unet_without_trajectory_learning\" : \"/home/mohammed-wa/PycharmProjects/MPILOT/summary/16/radial_0.0005_fixed/best_model.pt\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f0901e2-fd90-42f5-9ed7-9df664c21bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mohammed-wa/PycharmProjects/MPILOT/summary/16/radial_0.0005_0.075_0.001_0.001_changing_downwards_10_Unet/best_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2806914/127517582.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_file)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/mohammed-wa/PycharmProjects/MPILOT/summary/16/radial_0.0005_0.075_0.001_0.001_changing_downwards_10_Unet/best_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m _, Unet_with_trajectory_learning, optimizer_Unet_with_trajectory_learning, args_Unet_with_trajectory_learning = load_model(checkpoint_files[\u001b[33m\"\u001b[39m\u001b[33mUnet_with_trajectory_learning\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      2\u001b[39m _, Unet_with_trajectory_learning_with_PGD_Train_3norm, optimizer_Unet_with_trajectory_learning_with_PGD_Train_3norm, _ = load_model(checkpoint_files[\u001b[33m\"\u001b[39m\u001b[33mUnet_with_trajectory_learning_with_PGD_Train_3norm\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      3\u001b[39m _, Unet_with_trajectory_learning_with_noise_75_Train, optimizer_Unet_with_trajectory_learning_with_noise_75_Train, _ = load_model(checkpoint_files[\u001b[33m\"\u001b[39m\u001b[33mUnet_with_trajectory_learning_with_noise_75_Train\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(checkpoint_file)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model\u001b[39m(checkpoint_file):\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(checkpoint_file)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     checkpoint = torch.load(checkpoint_file)\n\u001b[32m     28\u001b[39m     args = checkpoint[\u001b[33m'\u001b[39m\u001b[33margs\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     29\u001b[39m     args.trajectory_learning = \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mpilot-py310/lib/python3.12/site-packages/torch/serialization.py:1319\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1317\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1321\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1322\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1323\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1324\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mpilot-py310/lib/python3.12/site-packages/torch/serialization.py:659\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    658\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    661\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mpilot-py310/lib/python3.12/site-packages/torch/serialization.py:640\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/mohammed-wa/PycharmProjects/MPILOT/summary/16/radial_0.0005_0.075_0.001_0.001_changing_downwards_10_Unet/best_model.pt'"
     ]
    }
   ],
   "source": [
    "_, Unet_with_trajectory_learning, optimizer_Unet_with_trajectory_learning, args_Unet_with_trajectory_learning = load_model(checkpoint_files[\"Unet_with_trajectory_learning\"])\n",
    "_, Unet_with_trajectory_learning_with_PGD_Train_3norm, optimizer_Unet_with_trajectory_learning_with_PGD_Train_3norm, _ = load_model(checkpoint_files[\"Unet_with_trajectory_learning_with_PGD_Train_3norm\"])\n",
    "_, Unet_with_trajectory_learning_with_noise_75_Train, optimizer_Unet_with_trajectory_learning_with_noise_75_Train, _ = load_model(checkpoint_files[\"Unet_with_trajectory_learning_with_noise_75_Train\"])\n",
    "_, Unet_with_trajectory_learning_with_noise_50_Train, optimizer_Unet_with_trajectory_learning_with_noise_50_Train, _ = load_model(checkpoint_files[\"Unet_with_trajectory_learning_with_noise_50_Train\"])\n",
    "_, Unet_with_trajectory_learning_with_noise_25_Train, optimizer_Unet_with_trajectory_learning_with_noise_25_Train, _ = load_model(checkpoint_files[\"Unet_with_trajectory_learning_with_noise_25_Train\"])\n",
    "_, Unet_with_trajectory_learning_with_image_1_Train, optimizer_Unet_with_trajectory_learning_with_image_1_Train, _ = load_model(checkpoint_files[\"Unet_with_trajectory_learning_with_image_1_Train\"])\n",
    "_, Unet_with_trajectory_learning_with_image_1_std_75_Train, optimizer_Unet_with_trajectory_learning_with_image_1_std_75_Train, _ = load_model(checkpoint_files[\"Unet_with_trajectory_learning_with_image_1_std_75_Train\"])\n",
    "_, Unet_without_trajectory_learning, optimizer_Unet_without_trajectory_learning, args_Unet_without_trajectory_learning = load_model(checkpoint_files[\"Unet_without_trajectory_learning\"])\n",
    "args = args_Unet_with_trajectory_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548cc9e1-627b-44ff-9740-5cfb670bdb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unet_without_trajectory_learning.module.subsampling.x.requires_grad_(True)\n",
    "Unet_without_trajectory_learning.module.subsampling.x = torch.nn.Parameter(Unet_without_trajectory_learning.module.subsampling.x, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2b360b-1ab7-4003-a494-7571babc8621",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    \"Unet_with_TL\": Unet_with_trajectory_learning,\n",
    "    \"Unet_TL_PGD_3norm\": Unet_with_trajectory_learning_with_PGD_Train_3norm,\n",
    "    \"Unet_TL_Noise_75\": Unet_with_trajectory_learning_with_noise_75_Train,\n",
    "    \"Unet_TL_Noise_50\": Unet_with_trajectory_learning_with_noise_50_Train,\n",
    "    \"Unet_TL_Noise_25\": Unet_with_trajectory_learning_with_noise_25_Train,\n",
    "    \"Unet_TL_Image_1\": Unet_with_trajectory_learning_with_image_1_Train,\n",
    "    \"Unet_TL_Image_1_Noise_75\": Unet_with_trajectory_learning_with_image_1_std_75_Train,\n",
    "    \"Unet_wo_TL\": Unet_without_trajectory_learning\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5a4911-e895-419b-9f53-3445e590b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict_2 = {\n",
    "    \"Unet_with_TL\": Unet_with_trajectory_learning,\n",
    "    \"Unet_wo_TL\": Unet_without_trajectory_learning\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d32966-5fd7-4191-91b4-8a58d9222c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_Unet_without_trajectory_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf19e30-b5ba-4562-9330-2efa2dbde711",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.seed\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae22a49-7c80-4b2c-ac09-e67687d4e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_nufft.nufft2 import nufft, nufft_adjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15620a71-3781-4a4e-88a4-3f4c85ee361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unet_without_trajectory_learning.module.subsampling.x.grad == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b990afad-6f58-44f9-b3aa-7c3d110ed114",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unet_with_trajectory_learning.module.subsampling.x.grad == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49973cd0-33ad-4bff-a57f-cac28d520fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unet_without_trajectory_learning.module.subsampling.x.requires_grad == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15bac5-1ff5-47e0-9ee0-05595d932125",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trajectory(Unet_with_trajectory_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e2ec4-a22e-4216-aec6-fdc84747b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trajectory(Unet_without_trajectory_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed638cf-a414-4eac-92fe-0c52638914ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trajectory(Unet_with_trajectory_learning_with_image_1_std_75_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e8bda0-ba6c-4611-9da0-4d7e445fdcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_and_plot(args, knee_loader, models_dict, title=\"PSNR Comparison Across Models\"):\n",
    "    %matplotlib inline\n",
    "    psnr_results = {}\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"Evaluating: {model_name}\")\n",
    "        _, psnr_value, _, _, _ = evaluate(args, 1, model, knee_loader)\n",
    "        psnr_results[model_name] = psnr_value\n",
    "    \n",
    "    # Plot the PSNR values\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bars = plt.bar(psnr_results.keys(), psnr_results.values(), color='skyblue')\n",
    "    plt.xlabel(\"Model Name\")\n",
    "    plt.ylabel(\"PSNR (dB)\")\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Add text labels on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, height, f\"{height:.2f}\", \n",
    "                 ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return psnr_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acba743-8c11-4a1a-a270-c13d1456d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# DNN Block (Dilated Conv)\n",
    "class DilatedConvBlock(nn.Module):\n",
    "    def __init__(self, channels, dilation):\n",
    "        super(DilatedConvBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=dilation, dilation=dilation),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "# Channel Attention\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_pool = F.adaptive_avg_pool2d(x, 1)\n",
    "        max_pool = F.adaptive_max_pool2d(x, 1)\n",
    "        attn = self.conv(avg_pool + max_pool)\n",
    "        return x * attn + x\n",
    "\n",
    "# Encoder Block\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, dilation):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.InstanceNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dnn = DilatedConvBlock(out_ch, dilation)\n",
    "        self.cam = ChannelAttention(out_ch)\n",
    "        self.down = nn.Conv2d(out_ch, out_ch, 3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.dnn(x)\n",
    "        skip = self.cam(x)\n",
    "        down = self.down(x)\n",
    "        return down, skip\n",
    "\n",
    "# Decoder Block\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, skip_ch, out_ch):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch + skip_ch, out_ch, 3, padding=1),\n",
    "            nn.InstanceNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.InstanceNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x_up = F.interpolate(x, size=skip.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        x_cat = torch.cat([x_up, skip], dim=1)\n",
    "        return self.conv(x_cat)\n",
    "\n",
    "# Generator with Multi-Scale Fusion\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=1, base_filters=64):\n",
    "        super(Generator, self).__init__()\n",
    "        f = base_filters\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = EncoderBlock(in_channels, f, dilation=1)   # 320  160\n",
    "        self.enc2 = EncoderBlock(f, f*2, dilation=2)           # 160  80\n",
    "        self.enc3 = EncoderBlock(f*2, f*4, dilation=3)         # 80  40\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(f*4, f*8, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(f*8, f*8, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.dec3 = DecoderBlock(f*8, f*4, f*4)  # 40  80\n",
    "        self.dec2 = DecoderBlock(f*4, f*2, f*2)  # 80  160\n",
    "        self.dec1 = DecoderBlock(f*2, f, f)      # 160  320\n",
    "\n",
    "        # Multi-scale fusion before output\n",
    "        self.fusion_conv = nn.Sequential(\n",
    "            nn.Conv2d(f + f*2 + f*4, f, 3, padding=1),\n",
    "            nn.InstanceNorm2d(f),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(f, in_channels, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1, s1 = self.enc1(x)   # 320  160\n",
    "        e2, s2 = self.enc2(e1)  # 160  80\n",
    "        e3, s3 = self.enc3(e2)  # 80  40\n",
    "\n",
    "        b = self.bottleneck(e3)\n",
    "\n",
    "        d3 = self.dec3(b, s3)   # 40  80\n",
    "        d2 = self.dec2(d3, s2)  # 80  160\n",
    "        d1 = self.dec1(d2, s1)  # 160  320\n",
    "\n",
    "        # Multi-scale upsample and fuse\n",
    "        u1 = F.interpolate(d1, size=(320, 320), mode='bilinear', align_corners=False)\n",
    "        u2 = F.interpolate(d2, size=(320, 320), mode='bilinear', align_corners=False)\n",
    "        u3 = F.interpolate(d3, size=(320, 320), mode='bilinear', align_corners=False)\n",
    "\n",
    "        fusion = torch.cat([u1, u2, u3], dim=1)  # [B, f + 2f + 4f, 320, 320]\n",
    "        out = self.fusion_conv(fusion)          # Final output\n",
    "        return out + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68af7d62-5d1b-41bc-8b02-8d525e762c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=1, base_filters=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # Input: (in_channels) x 320 x 320\n",
    "            nn.Conv2d(in_channels, base_filters, kernel_size=4, stride=2, padding=1),  # 320  160\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(base_filters, base_filters*2, kernel_size=4, stride=2, padding=1, bias=False),  # 160  80\n",
    "            nn.BatchNorm2d(base_filters*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(base_filters*2, base_filters*4, kernel_size=4, stride=2, padding=1, bias=False),  # 80  40\n",
    "            nn.BatchNorm2d(base_filters*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(base_filters*4, base_filters*8, kernel_size=4, stride=2, padding=1, bias=False),  # 40  20\n",
    "            nn.BatchNorm2d(base_filters*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # Final classifier: 20  17 (since kernel=4, stride=1, no padding)\n",
    "            nn.Conv2d(base_filters*8, 1, kernel_size=4, stride=1, padding=0),\n",
    "            nn.Sigmoid()  # Output: probability map\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)  # shape: [B, 1, H', W']\n",
    "        return out.view(x.size(0), -1).mean(dim=1)  # average output over spatial map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc38201-19c5-49aa-a880-f4b81a445f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 20\n",
    "args.sample_rate = 0.02\n",
    "dev_loader = create_data_loaders(args)\n",
    "#args.sample_rate = 0.01\n",
    "#knee_loader = create_data_loaders(args, knee = True)\n",
    "#args.sample_rate = 0.05\n",
    "#m4raw_loader = create_data_loaders(args, m4raw = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e285d48-0fe4-40f5-aad8-77ba8e4057a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del iter\n",
    "first_input_brain_fastmri, first_target_brain_fastmri = get_first_batch(dev_loader)\n",
    "#first_input_brain_knee, first_target_brain_knee = get_first_batch(knee_loader)\n",
    "#first_input_brain_m4raw, first_target_brain_m4rwa = get_first_batch(m4raw_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbd423-ab35-4286-a202-0859db532067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4585ac-2a2a-4e4e-912a-9d4e0b34bce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Unet_with_trajectory_learning.module.subsampling(first_input_brain_fastmri.unsqueeze(1).float().to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695597f2-ff68-4a26-bd87-b74de633bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e169e-b252-44c5-8465-534986ec2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Unet_with_trajectory_learning.module.subsampling(first_input_brain_fastmri.unsqueeze(1).float().to(\"cuda\"))\n",
    "undersampled = transforms.root_sum_of_squares(transforms.complex_abs(input), dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82e44b-0819-4d68-885b-d7051123a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled.shape\n",
    "show_image(undersampled[1].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d8afe-b0f8-4450-8109-990441866b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMUSReconstructionNet(undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed4bccb-e6b3-4cc5-abef-3b8a9bd50102",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.normalize_instance(first_input_brain_fastmri, eps=1e-11)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179425e7-f768-4a37-8d77-8ae3f6e510d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.rec_models.vit_model import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f5852-1479-4d0d-9d5b-1ff9ccbc6975",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = VisionTransformer().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8cb3cf-265b-4e6c-aaff-734c7da5f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMUSReconstructionNet = HUMUSBlock(img_size=(320, 320), in_chans =1, out_chans = 1, num_blocks = 1, window_size = 10, embed_dim = 66).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa4556-aab7-4333-b77f-43c72ec5a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator().to(\"cuda\")\n",
    "D = Discriminator().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd4d00-7f5a-41aa-8398-54b276c2273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage\n",
    "total_params_generator = count_parameters(G)\n",
    "total_params_discriminator = count_parameters(D)\n",
    "print(f\"Total trainable parameters: {total_params_generator:,}\")\n",
    "print(f\"Total trainable parameters: {total_params_discriminator:,}\")\n",
    "\n",
    "size_mb_generator = (total_params_generator * 4) / (1024 ** 2)\n",
    "size_mb_discriminator = (total_params_discriminator * 4) / (1024 ** 2)\n",
    "print(f\"Model size: {size_mb_generator:.2f} MB\")\n",
    "print(f\"Model size: {size_mb_discriminator:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c8af23-a787-4967-afb0-244d33a76121",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef0cf6-fa93-4466-89b1-9ff9000675f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 1894704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c685ac26-74d9-4d6e-8206-8f8278b9bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5063a8e7-7f30-4a1a-b045-94dae015a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = vit(undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3274952b-239b-42c5-8042-3dfc64c21145",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a695ab5-32ac-41e7-a865-bfb2fa233436",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = G(undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e0d252-5277-4441-a509-bce769c673e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(fake[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a70a13-107a-4eb0-a472-bc3e265c043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_output = D(undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845b5a9-0ff7-474b-8c72-d2c2e28bf604",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e74b1-02de-4ff7-bd7b-0f1b98e4cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio as sk_psnr\n",
    "from skimage.metrics import structural_similarity as sk_ssim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "G = HUMUSReconstructionNet\n",
    "\n",
    "# Loss function\n",
    "pixel_loss_fn = nn.MSELoss()\n",
    "\n",
    "# Optimizer and scheduler\n",
    "g_optimizer = optim.Adam(G.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
    "g_scheduler = optim.lr_scheduler.StepLR(g_optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "def normalize(img):\n",
    "    return (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "\n",
    "# Training settings\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    G.train()\n",
    "    for iter, data in enumerate(dev_loader):\n",
    "        und_img, full_img, _, _, _ = data\n",
    "\n",
    "        und_img = Unet_with_trajectory_learning_with_image_1_std_75_Train.module.subsampling(\n",
    "            und_img.unsqueeze(1).float().to(\"cuda\")\n",
    "        )\n",
    "        und_img = transforms.root_sum_of_squares(transforms.complex_abs(und_img), dim=1).unsqueeze(1)\n",
    "        und_img = normalize(und_img).to(device)\n",
    "        full_img = normalize(full_img.unsqueeze(1)).to(device)\n",
    "\n",
    "        G.zero_grad()\n",
    "        fake_img = normalize(G(und_img))\n",
    "        show_image(fake_img[0])\n",
    "        loss_pixel = pixel_loss_fn(fake_img, full_img)\n",
    "        loss_pixel.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        if iter % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Batch {iter} Pixel_Loss: {loss_pixel.item():.4f}\")\n",
    "\n",
    "    g_scheduler.step()\n",
    "\n",
    "    # Validation\n",
    "    G.eval()\n",
    "    psnr_vals, ssim_vals, mse_vals = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for iter, data in enumerate(dev_loader):\n",
    "            und_img, full_img, _, _, _ = data\n",
    "\n",
    "            und_img = Unet_with_trajectory_learning.module.subsampling(\n",
    "                und_img.unsqueeze(1).float().to(\"cuda\")\n",
    "            )\n",
    "            und_img = transforms.root_sum_of_squares(transforms.complex_abs(und_img), dim=1).unsqueeze(1)\n",
    "            und_val = und_img.to(device)\n",
    "            full_val = full_img.unsqueeze(1).to(device)\n",
    "\n",
    "            recon_val = G(und_val)\n",
    "            recon_val = torch.clamp(recon_val, 0.0, 1.0)\n",
    "\n",
    "            rec_np = recon_val.cpu().numpy().astype(np.float32)\n",
    "            full_np = full_val.cpu().numpy().astype(np.float32)\n",
    "\n",
    "            for i in range(rec_np.shape[0]):\n",
    "                rec_img = rec_np[i, 0]\n",
    "                full_img_np = full_np[i, 0]\n",
    "                psnr_vals.append(sk_psnr(full_img_np, rec_img, data_range=1.0))\n",
    "                ssim_vals.append(sk_ssim(full_img_np, rec_img, data_range=1.0))\n",
    "                mse_vals.append(np.mean((full_img_np - rec_img) ** 2))\n",
    "\n",
    "    print(f\"Validation PSNR: {np.mean(psnr_vals):.2f}, \"\n",
    "          f\"SSIM: {np.mean(ssim_vals):.4f}, MSE: {np.mean(mse_vals):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c091e-5e7f-4898-bc43-b3aada5223bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "del G\n",
    "del D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33b2c2-cc77-46a0-bbb7-afb2a93ebabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio as sk_psnr\n",
    "from skimage.metrics import structural_similarity as sk_ssim\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "G = Generator().to(\"cuda\")\n",
    "D = Discriminator().to(\"cuda\")\n",
    "def normalize(img):\n",
    "    return (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ---- VGG-based perceptual loss ----\n",
    "class VGGLoss(nn.Module):\n",
    "    def __init__(self, layer='relu3_3'):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features[:16].eval()\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vgg = vgg.to(device)\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).to(device).view(1, 3, 1, 1)\n",
    "        self.std  = torch.tensor([0.229, 0.224, 0.225]).to(device).view(1, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = x.repeat(1, 3, 1, 1)  # 1-channel to 3-channel\n",
    "        y = y.repeat(1, 3, 1, 1)\n",
    "        x = (x - self.mean) / self.std\n",
    "        y = (y - self.mean) / self.std\n",
    "        return F.l1_loss(self.vgg(x), self.vgg(y))\n",
    "\n",
    "# ---- Loss functions ----\n",
    "pixel_loss_fn = nn.MSELoss()\n",
    "adv_loss_fn = nn.BCELoss()\n",
    "perceptual_loss = VGGLoss()\n",
    "\n",
    "def freq_loss(x, y):\n",
    "    fx = torch.fft.fft2(x.squeeze(1)).abs()\n",
    "    fy = torch.fft.fft2(y.squeeze(1)).abs()\n",
    "    return F.mse_loss(fx, fy)\n",
    "\n",
    "# ---- Optimizers ----\n",
    "g_optimizer = optim.Adam(G.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
    "d_optimizer = optim.Adam(D.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
    "\n",
    "# ---- LR schedulers ----\n",
    "g_scheduler = optim.lr_scheduler.StepLR(g_optimizer, step_size=10, gamma=0.5)\n",
    "d_scheduler = optim.lr_scheduler.StepLR(d_optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# ---- Training hyperparameters ----\n",
    "num_epochs = 50\n",
    "lambda_pixel = 1.0\n",
    "lambda_freq = 1.0\n",
    "lambda_perceptual = 0\n",
    "lambda_adv = 0.001\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    G.train()\n",
    "    D.train()\n",
    "\n",
    "    for iter, data in enumerate(dev_loader):\n",
    "        und_img, full_img, mean, std, norm = data\n",
    "        und_img = Unet_with_trajectory_learning_with_image_1_std_75_Train.module.subsampling(und_img.unsqueeze(1).float().to(\"cuda\"))\n",
    "        und_img = transforms.root_sum_of_squares(transforms.complex_abs(und_img), dim=1).unsqueeze(1)\n",
    "        und_img = und_img.to(device)\n",
    "        full_img = full_img.unsqueeze(1).to(device)\n",
    "\n",
    "        und_img = normalize(und_img)\n",
    "        full_img = normalize(full_img)\n",
    "        #und_img, mean, std = transforms.normalize_instance(und_img, eps=1e-11)\n",
    "\n",
    "        # --- Train Discriminator ---\n",
    "        D.zero_grad()\n",
    "        real_labels = torch.ones(und_img.size(0), device=device)\n",
    "        fake_labels = torch.zeros(und_img.size(0), device=device)\n",
    "\n",
    "        #print(full_img.shape, und_img.shape)\n",
    "        d_real = D(full_img)\n",
    "        d_loss_real = adv_loss_fn(d_real, real_labels)\n",
    "\n",
    "        fake_img = G(und_img).detach()\n",
    "        show_image(fake_img[0])\n",
    "        d_fake = D(fake_img)\n",
    "        d_loss_fake = adv_loss_fn(d_fake, fake_labels)\n",
    "\n",
    "        d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # --- Train Generator ---\n",
    "        G.zero_grad()\n",
    "        fake_img = G(und_img)\n",
    "\n",
    "        loss_pixel = pixel_loss_fn(fake_img, full_img)\n",
    "        loss_freq_val = freq_loss(fake_img, full_img)\n",
    "        loss_perc = perceptual_loss(fake_img, full_img)\n",
    "        pred_fake = D(fake_img)\n",
    "        loss_adv = adv_loss_fn(pred_fake, real_labels)\n",
    "\n",
    "        g_loss = (lambda_pixel * loss_pixel +\n",
    "                  lambda_freq * loss_freq_val +\n",
    "                  lambda_perceptual * loss_perc +\n",
    "                  lambda_adv * loss_adv)\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        if iter % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Batch {iter} \"\n",
    "                  f\"D_loss: {d_loss.item():.4f} G_loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    g_scheduler.step()\n",
    "    d_scheduler.step()\n",
    "\n",
    "    # --- Validation ---\n",
    "    G.eval()\n",
    "    psnr_vals, ssim_vals, mse_vals = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for iter, data in enumerate(dev_loader):\n",
    "            und_img, full_img, mean, std, norm = data\n",
    "            und_img = Unet_with_trajectory_learning.module.subsampling(und_img.unsqueeze(1).float().to(\"cuda\"))\n",
    "            und_img = transforms.root_sum_of_squares(transforms.complex_abs(und_img), dim=1).unsqueeze(1)\n",
    "            und_val = und_img.to(device)\n",
    "            full_val = full_img.unsqueeze(1).to(device)\n",
    "            recon_val = G(und_val)\n",
    "            recon_val = torch.clamp(recon_val, 0.0, 1.0)\n",
    "\n",
    "            rec_np = recon_val.cpu().numpy().astype(np.float32)\n",
    "            full_np = full_val.cpu().numpy().astype(np.float32)\n",
    "\n",
    "            for i in range(rec_np.shape[0]):\n",
    "                rec_img = rec_np[i, 0]\n",
    "                full_img_np = full_np[i, 0]\n",
    "                psnr_vals.append(sk_psnr(full_img_np, rec_img, data_range=1.0))\n",
    "                ssim_vals.append(sk_ssim(full_img_np, rec_img, data_range=1.0))\n",
    "                mse_vals.append(np.mean((full_img_np - rec_img) ** 2))\n",
    "\n",
    "    print(f\"Validation PSNR: {np.mean(psnr_vals):.2f}, \"\n",
    "          f\"SSIM: {np.mean(ssim_vals):.4f}, MSE: {np.mean(mse_vals):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323238b8-f106-42cf-98e0-7811692ec71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_input_brain_fastmri.unsqueeze(1).to(\"cuda\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f3ccd9-2885-4ac9-b560-d15e0e3f4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_trajectory_noise_effects(models_dict, first_input_brain_fastmri, std_list=[25, 50, 75, 100], clip_range=(-160, 160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb087d8-ce4d-4ae8-b5c6-18fab7b6d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attack_on_models(models_dict, first_input_brain_fastmri, first_target_brain_fastmri, epsilon=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab758a-ef09-48f7-b9ad-30e83d0e6153",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_results_brain = evaluate_and_plot(args, dev_loader, models_dict, title=\"PSNR Comparison Across Models With FastMRI Brain Dataset\")\n",
    "psnr_results_knee = evaluate_and_plot(args, knee_loader, models_dict, title=\"PSNR Comparison Across Models With FastMRI Knee Dataset\")\n",
    "psnr_results_knee_tuned = evaluate_and_plot(args, knee_loader, models_knee, title=\"PSNR Comparison Across knee tuned Models With FastMRI Knee Dataset\")\n",
    "psnr_results_m4raw = evaluate_and_plot(args, m4raw_loader, models_dict, title=\"PSNR Comparison Across Models With M4raw Brain Dataset\")\n",
    "psnr_results_m4raw_tuned = evaluate_and_plot(args, m4raw_loader, models_m4raw, title=\"PSNR Comparison Across m4raw tuned Models With M4raw Brain Datasett\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b1a108-1bba-4041-90a2-edbe034cf386",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_results_knee = evaluate_and_plot(args, m4raw_loader, models_dict, title=\"PSNR Comparison Across Models With M4raw Brain Dataset\")\n",
    "psnr_results_knee = evaluate_and_plot(args, knee_loader, models_m4raw, title=\"PSNR Comparison Across Models tuned on the m4raw With FastMRI Knee Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a2aa1-fcbb-4b17-9994-d48d96a4c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, Unet_with_trajectory_learning, optimizer_Unet_with_trajectory_learning, args_Unet_with_trajectory_learning = load_model(checkpoint_files[\"Unet_with_trajectory_learning\"])\n",
    "_, Unet_with_trajectory_learning_with_PGD_Train_3norm, optimizer_Unet_with_trajectory_learning_with_PGD_Train_3norm, _ = load_model(checkpoint_files[\"Unet_with_trajectory_learning_with_PGD_Train_3norm\"])\n",
    "_, Unet_with_trajectory_learning_with_noise_75_Train, optimizer_Unet_with_trajectory_learning_with_noise_75_Train, _ = load_model(checkpoint_files[\"Unet_with_trajectory_learning_with_noise_75_Train\"])\n",
    "_, Unet_with_trajectory_learning_with_noise_50_Train, optimizer_Unet_with_trajectory_learning_with_noise_50_Train, _ = load_model(checkpoint_files[\"Unet_with_trajectory_learning_with_noise_50_Train\"])\n",
    "_, Unet_with_trajectory_learning_with_noise_25_Train, optimizer_Unet_with_trajectory_learning_with_noise_25_Train, _ = load_model(checkpoint_files[\"Unet_with_trajectory_learning_with_noise_25_Train\"])\n",
    "_, Unet_with_trajectory_learning_with_image_1_Train, optimizer_Unet_with_trajectory_learning_with_image_1_Train, _ = load_model(checkpoint_files[\"Unet_with_trajectory_learning_with_image_1_Train\"])\n",
    "_, Unet_with_trajectory_learning_with_image_1_std_75_Train, optimizer_Unet_with_trajectory_learning_with_image_1_std_75_Train, _ = load_model(checkpoint_files[\"Unet_with_trajectory_learning_with_image_1_std_75_Train\"])\n",
    "_, Unet_without_trajectory_learning, optimizer_Unet_without_trajectory_learning, _ = load_model(checkpoint_files[\"Unet_without_trajectory_learning\"])\n",
    "args = args_Unet_with_trajectory_learning\n",
    "models_dict = {\n",
    "    \"Unet_with_TL\": Unet_with_trajectory_learning,\n",
    "    \"Unet_TL_PGD_3norm\": Unet_with_trajectory_learning_with_PGD_Train_3norm,\n",
    "    \"Unet_TL_Noise_75\": Unet_with_trajectory_learning_with_noise_75_Train,\n",
    "    \"Unet_TL_Noise_50\": Unet_with_trajectory_learning_with_noise_50_Train,\n",
    "    \"Unet_TL_Noise_25\": Unet_with_trajectory_learning_with_noise_25_Train,\n",
    "    \"Unet_TL_Image_1\": Unet_with_trajectory_learning_with_image_1_Train,\n",
    "    \"Unet_TL_Image_1_Noise_75\": Unet_with_trajectory_learning_with_image_1_std_75_Train,\n",
    "    \"Unet_wo_TL\": Unet_without_trajectory_learning\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24daf242-f959-452b-9b19-2c0254732b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, Unet_with_trajectory_learning, optimizer_Unet_with_trajectory_learning, args_Unet_with_trajectory_learning = load_model(checkpoint_files[\"Unet_with_trajectory_learning\"])\n",
    "_, Unet_with_trajectory_learning_with_PGD_Train_3norm, optimizer_Unet_with_trajectory_learning_with_PGD_Train_3norm, _ = load_model(checkpoint_files[\"Unet_with_trajectory_learning_with_PGD_Train_3norm\"])\n",
    "_, Unet_with_trajectory_learning_with_noise_75_Train, optimizer_Unet_with_trajectory_learning_with_noise_75_Train, _ = load_model(checkpoint_files[\"Unet_with_trajectory_learning_with_noise_75_Train\"])\n",
    "_, Unet_with_trajectory_learning_with_noise_50_Train, optimizer_Unet_with_trajectory_learning_with_noise_50_Train, _ = load_model(checkpoint_files[\"Unet_with_trajectory_learning_with_noise_50_Train\"])\n",
    "_, Unet_with_trajectory_learning_with_noise_25_Train, optimizer_Unet_with_trajectory_learning_with_noise_25_Train, _ = load_model(checkpoint_files[\"Unet_with_trajectory_learning_with_noise_25_Train\"])\n",
    "_, Unet_with_trajectory_learning_with_image_1_Train, optimizer_Unet_with_trajectory_learning_with_image_1_Train, _ = load_model(checkpoint_files[\"Unet_with_trajectory_learning_with_image_1_Train\"])\n",
    "_, Unet_with_trajectory_learning_with_image_1_std_75_Train, optimizer_Unet_with_trajectory_learning_with_image_1_std_75_Train, _ = load_model(checkpoint_files[\"Unet_with_trajectory_learning_with_image_1_std_75_Train\"])\n",
    "_, Unet_without_trajectory_learning, optimizer_Unet_without_trajectory_learning, _ = load_model(checkpoint_files[\"Unet_without_trajectory_learning\"])\n",
    "args = args_Unet_with_trajectory_learning\n",
    "models_dict = {\n",
    "    \"Unet_with_TL\": Unet_with_trajectory_learning,\n",
    "    \"Unet_TL_PGD_3norm\": Unet_with_trajectory_learning_with_PGD_Train_3norm,\n",
    "    \"Unet_TL_Noise_75\": Unet_with_trajectory_learning_with_noise_75_Train,\n",
    "    \"Unet_TL_Noise_50\": Unet_with_trajectory_learning_with_noise_50_Train,\n",
    "    \"Unet_TL_Noise_25\": Unet_with_trajectory_learning_with_noise_25_Train,\n",
    "    \"Unet_TL_Image_1\": Unet_with_trajectory_learning_with_image_1_Train,\n",
    "    \"Unet_TL_Image_1_Noise_75\": Unet_with_trajectory_learning_with_image_1_std_75_Train,\n",
    "    \"Unet_wo_TL\": Unet_without_trajectory_learning\n",
    "}\n",
    "for model_name, model in models_dict.items():\n",
    "    print(f\"\\nTraining {model_name}...\\n\")\n",
    "\n",
    "    # Access the inner model (supports both nn.DataParallel and plain models)\n",
    "    inner_model = model.module if hasattr(model, \"module\") else model\n",
    "\n",
    "    # Create optimizer for the reconstruction_model submodule\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [{'params': inner_model.reconstruction_model.parameters()}],\n",
    "        lr=args.lr\n",
    "    )\n",
    "\n",
    "    for epoch in range(5):\n",
    "        print(f\"Epoch {epoch + 1}/5\")\n",
    "        train_epoch(args, model, m4raw_loader, optimizer)\n",
    "        os.makedirs(f\"./{model_name}\", exist_ok=True)\n",
    "        torch.save({'model': model.state_dict()},f\"./{model_name}\"+ '/m4raw_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f954347-f23c-41b2-8f2e-f20ceabaa662",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models_dict.items():\n",
    "    print(f\"\\nTraining {model_name}...\\n\")\n",
    "\n",
    "    # Access the inner model (supports both nn.DataParallel and plain models)\n",
    "    inner_model = model.module if hasattr(model, \"module\") else model\n",
    "\n",
    "    # Create optimizer for the reconstruction_model submodule\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [{'params': inner_model.reconstruction_model.parameters()}],\n",
    "        lr=args.lr\n",
    "    )\n",
    "\n",
    "    for epoch in range(5):\n",
    "        print(f\"Epoch {epoch + 1}/5\")\n",
    "        train_epoch(args, model, knee_loader, optimizer)\n",
    "        os.makedirs(f\"./{model_name}\", exist_ok=True)\n",
    "        torch.save({'model': model.state_dict()},f\"./{model_name}\"+ '/knee_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e040bc-1233-479b-8955-a5158a8e3513",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models_dict.items():\n",
    "    print(f\"\\nTraining {model_name}...\\n\")\n",
    "\n",
    "    # Access the inner model (supports both nn.DataParallel and plain models)\n",
    "    inner_model = model.module if hasattr(model, \"module\") else model\n",
    "\n",
    "    # Create optimizer for the reconstruction_model submodule\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [{'params': inner_model.reconstruction_model.parameters()}],\n",
    "        lr=args.lr\n",
    "    )\n",
    "\n",
    "    for epoch in range(5):\n",
    "        print(f\"Epoch {epoch + 1}/5\")\n",
    "        train_epoch(args, model, m4raw_loader, optimizer)\n",
    "        os.makedirs(f\"./{model_name}\", exist_ok=True)\n",
    "        torch.save({'model': model.state_dict()},f\"./{model_name}\"+ '/m4raw_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ca5ec-113d-4458-914b-7584578f3813",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(input)\n",
    "show_image(first_target_brain_m4raw[0])\n",
    "show_image(Unet_with_trajectory_learning(first_input_brain_m4raw[0].unsqueeze(0).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7c184-41f6-4aee-905e-1015dc6227ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_and_grid(first_target_brain_m4raw, \"Target Brain (M4Raw)\")\n",
    "normalize_and_grid(Unet_with_trajectory_learning(first_input_brain_m4raw.unsqueeze(0)), \"UNet Output (Brain M4Raw)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c231a-9153-42fa-897f-e426be1db728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import common\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def smooth_curve(values, window=5, polyorder=2):\n",
    "    \"\"\"Applies Savitzky-Golay filter for smooth curves.\"\"\"\n",
    "    if len(values) >= window:\n",
    "        return savgol_filter(values, window, polyorder)\n",
    "    else:\n",
    "        return values  # No smoothing if window is too large\n",
    "\n",
    "def compare_models_psnr_vs_noise(args, models_dict, dataloader, smooth_window=7):\n",
    "    \"\"\"\n",
    "    Compares PSNR vs noise standard deviation for multiple models with smoothing.\n",
    "    \n",
    "    Args:\n",
    "        models_dict: Dictionary of {'model_name': model} pairs\n",
    "        dataloader: DataLoader providing (input_batch, target_batch)\n",
    "        smooth_window: Window size for smoothing (default: 7)\n",
    "    \"\"\"\n",
    "    noise_stds = np.linspace(0, 320, 10)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    fig.suptitle(\"Model Comparison: PSNR vs Noise Standard Deviation\", y=1.02, fontsize=16)\n",
    "\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(models_dict)))\n",
    "    markers = ['o', 's', '^', 'v', '<', '>', 'p', '*', 'h', 'H', 'D', 'd']\n",
    "    line_styles = ['-', '--', ':', '-.']\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for i, (model_name, model) in enumerate(models_dict.items()):\n",
    "        print(f\"\\nTesting model: {model_name}\")\n",
    "        psnr_values_per_std = {std: [] for std in noise_stds}\n",
    "\n",
    "        initial_trajectory = model.module.subsampling.x.clone()\n",
    "\n",
    "        for std in tqdm(noise_stds):\n",
    "            total_psnr = 0\n",
    "            num_batches = 0\n",
    "\n",
    "            for input_batch, target_batch, _, _, _ in dataloader:\n",
    "                input_batch, target_batch = input_batch.to(args.device), target_batch.to(args.device)\n",
    "                noise = torch.randn_like(initial_trajectory) * std\n",
    "                noisy_traj = torch.clamp(initial_trajectory + noise, -160, 160)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    model.module.subsampling.x.data = noisy_traj\n",
    "                    output = model(input_batch.unsqueeze(1))\n",
    "                    psnr_val = common.evaluate.psnr(target_batch.cpu().numpy(), output.detach().cpu().numpy())\n",
    "\n",
    "                total_psnr += psnr_val\n",
    "                num_batches += 1\n",
    "\n",
    "            avg_psnr = total_psnr / num_batches\n",
    "            psnr_values_per_std[std].append(avg_psnr)\n",
    "\n",
    "        # Restore original trajectory\n",
    "        model.module.subsampling.x.data = initial_trajectory\n",
    "\n",
    "        # Average PSNR values and apply smoothing\n",
    "        avg_psnr_values = [np.mean(psnr_values_per_std[std]) for std in noise_stds]\n",
    "        smoothed_psnr = smooth_curve(avg_psnr_values, window=smooth_window)\n",
    "\n",
    "        ax.plot(noise_stds, smoothed_psnr, \n",
    "                label=model_name,\n",
    "                color=colors[i % len(colors)],\n",
    "                marker=markers[i % len(markers)],\n",
    "                linestyle=line_styles[i % len(line_styles)])\n",
    "    \n",
    "    ax.set_xlabel('Noise Standard Deviation', fontsize=12)\n",
    "    ax.set_ylabel('PSNR (dB)', fontsize=12)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax.legend(fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc583a04-5405-46f2-af2b-f7cadd268c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare_models_psnr_vs_noise(\n",
    "    args,\n",
    "    models_dict=models_dict,\n",
    "    dataloader = dev_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d11c3-993a-4dc4-9111-2c85c0273de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare_models_psnr_vs_noise(\n",
    "    args,\n",
    "    models_dict=models_dict_2,\n",
    "    dataloader = dev_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f7991b-dd52-4bbc-9da1-1f190de91135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import common\n",
    "\n",
    "def compare_models_psnr_vs_noise_input(args, models_dict, dataloader, noise_range=(0, 5), num_points=10):\n",
    "    \"\"\"\n",
    "    Compares PSNR vs noise standard deviation for multiple models with noise added to input images.\n",
    "    \n",
    "    Args:\n",
    "        models_dict: Dictionary of {'model_name': model} pairs\n",
    "        dataloader: DataLoader providing (input_batch, target_batch)\n",
    "        noise_range: Tuple (min_std, max_std) for noise range\n",
    "        num_points: Number of noise levels to test\n",
    "    \"\"\"\n",
    "    # Setup noise standard deviations to test\n",
    "    noise_stds = np.linspace(noise_range[0], noise_range[1], num_points)\n",
    "    \n",
    "    # Initialize plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    fig.suptitle(\"Model Comparison: PSNR vs Input Noise Standard Deviation\", \n",
    "                 y=1.02, fontsize=16, weight='bold')\n",
    "    \n",
    "    # Dynamic styling for models\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(models_dict)))\n",
    "    markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h']\n",
    "    \n",
    "    # Store results\n",
    "    results = {}\n",
    "\n",
    "    # Test each model\n",
    "    for idx, (model_name, model) in enumerate(models_dict.items()):\n",
    "        print(f\"\\nTesting model: {model_name}\")\n",
    "        psnr_values_per_std = {std: [] for std in noise_stds}\n",
    "\n",
    "        for std in tqdm(noise_stds, desc=f'Noise levels for {model_name}'):\n",
    "            total_psnr = 0\n",
    "            num_batches = 0\n",
    "\n",
    "            for input_batch, target_batch, _, _, _ in dataloader:\n",
    "                input_batch, target_batch = input_batch.to(args.device), target_batch.to(args.device)\n",
    "                \n",
    "                # Generate Gaussian noise and apply it\n",
    "                noise = torch.randn_like(input_batch) * std\n",
    "                noisy_input = input_batch + noise\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = model(noisy_input.unsqueeze(1))\n",
    "                    psnr_val = common.evaluate.psnr(target_batch.cpu().numpy(), \n",
    "                                                    output.detach().cpu().numpy())\n",
    "\n",
    "                total_psnr += psnr_val\n",
    "                num_batches += 1\n",
    "\n",
    "            # Store average PSNR for this noise level\n",
    "            avg_psnr = total_psnr / num_batches\n",
    "            psnr_values_per_std[std].append(avg_psnr)\n",
    "\n",
    "        # Compute final averaged PSNR values for the model\n",
    "        avg_psnr_values = [np.mean(psnr_values_per_std[std]) for std in noise_stds]\n",
    "        results[model_name] = list(zip(noise_stds, avg_psnr_values))\n",
    "\n",
    "        # Plot results with unique style\n",
    "        ax.plot(noise_stds, avg_psnr_values, \n",
    "                color=colors[idx],\n",
    "                marker=markers[idx % len(markers)],\n",
    "                linestyle='-',\n",
    "                linewidth=2,\n",
    "                markersize=8,\n",
    "                label=model_name.replace('_', ' ').title())\n",
    "    \n",
    "    # Enhanced plot formatting\n",
    "    ax.set_xlabel('Input Noise Standard Deviation ()', fontsize=12)\n",
    "    ax.set_ylabel('PSNR (dB)', fontsize=12)\n",
    "    ax.grid(True, linestyle=':', alpha=0.7)\n",
    "    \n",
    "    # Auto-adjust y-axis limits based on data\n",
    "    all_psnrs = np.concatenate([np.array(v)[:,1] for v in results.values()])\n",
    "    y_pad = 0.05 * (np.max(all_psnrs) - np.min(all_psnrs))\n",
    "    ax.set_ylim(np.min(all_psnrs) - y_pad, np.max(all_psnrs) + y_pad)\n",
    "    \n",
    "    # Legend styling\n",
    "    legend = ax.legend(fontsize=10, framealpha=1, loc='upper right', bbox_to_anchor=(1, 1))\n",
    "    legend.get_frame().set_edgecolor('0.5')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547c0c11-fec3-4161-9dff-a05088c87667",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare_models_psnr_vs_noise_input(\n",
    "    args,\n",
    "    models_dict=models_dict,\n",
    "    dataloader=dev_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62adf335-37e2-4cbb-bafd-e71f839dea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compare_psnr_vs_epsilon_dataloader(models_dict, dataloader, \n",
    "                                       max_epsilon=20.0, num_points=30, \n",
    "                                       steps=10, norm='linf', ylim=None):\n",
    "    \"\"\"\n",
    "    Compare PSNR vs epsilon averaged over an entire dataloader.\n",
    "\n",
    "    Args:\n",
    "        models_dict: Dictionary of {'model_name': model} pairs\n",
    "        dataloader: PyTorch DataLoader yielding (input_tensor, target_tensor)\n",
    "        max_epsilon: Maximum epsilon value\n",
    "        num_points: Number of epsilon values to test\n",
    "        steps: PGD attack steps\n",
    "        norm: Attack norm ('l1', 'l2', 'linf')\n",
    "        ylim: Optional y-axis limits (ymin, ymax)\n",
    "    \"\"\"\n",
    "    epsilons = np.linspace(0, max_epsilon, num_points)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(models_dict)))\n",
    "    markers = ['o', 's', '^', 'D', 'v', '*', 'p', 'X']\n",
    "\n",
    "    results = {}\n",
    "    failed_models = []\n",
    "\n",
    "    for i, (model_name, model) in enumerate(models_dict.items()):\n",
    "        model.eval()\n",
    "        psnrs = []\n",
    "        failed = False\n",
    "\n",
    "        for eps in tqdm(epsilons, desc=f'{model_name}', leave=False):\n",
    "            try:\n",
    "                psnr_accumulator = []\n",
    "                \n",
    "                for input_tensor, target_tensor, _, _, _ in dataloader:\n",
    "                    input_tensor = input_tensor.to(next(model.parameters()).device)\n",
    "                    target_tensor = target_tensor.to(next(model.parameters()).device)\n",
    "\n",
    "                    if eps == 0:\n",
    "                        with torch.no_grad():\n",
    "                            output = model(input_tensor.unsqueeze(1))\n",
    "                            batch_psnr = psnr(target_tensor.cpu().numpy(), \n",
    "                                              output.detach().cpu().numpy())\n",
    "                    else:\n",
    "                        _, batch_psnr = pgd_attack_with_trajectory(\n",
    "                            model, input_tensor, target_tensor,\n",
    "                            epsilon=eps, alpha=eps/steps, steps=steps, norm=norm\n",
    "                        )\n",
    "\n",
    "                    psnr_accumulator.append(batch_psnr)\n",
    "\n",
    "                mean_psnr = np.mean(psnr_accumulator)\n",
    "                psnrs.append(mean_psnr)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Model {model_name} failed at epsilon {eps}: {str(e)}\")\n",
    "                failed = True\n",
    "                break\n",
    "\n",
    "        if failed:\n",
    "            failed_models.append(model_name)\n",
    "        else:\n",
    "            results[model_name] = psnrs\n",
    "            plt.plot(epsilons, psnrs,\n",
    "                     color=colors[i],\n",
    "                     marker=markers[i % len(markers)],\n",
    "                     linestyle='-',\n",
    "                     linewidth=2,\n",
    "                     markersize=8,\n",
    "                     label=model_name.replace('_', ' ').title())\n",
    "\n",
    "    ax.set_xlabel('Attack Strength ()', fontsize=12)\n",
    "    ax.set_ylabel('PSNR (dB)', fontsize=12)\n",
    "    ax.set_title(f'Adversarial Robustness (Norm: {norm.upper()})', fontsize=14, pad=15)\n",
    "\n",
    "    if ylim is None:\n",
    "        all_psnrs = np.concatenate(list(results.values())) if results else [0]\n",
    "        ymin = max(0, np.min(all_psnrs) - 5)\n",
    "        ymax = min(100, np.max(all_psnrs) + 5)\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "    else:\n",
    "        ax.set_ylim(ylim)\n",
    "\n",
    "    ax.grid(True, linestyle=':', alpha=0.7)\n",
    "    ax.set_xticks(np.linspace(0, max_epsilon, 11))\n",
    "    ax.set_yticks(np.linspace(ax.get_ylim()[0], ax.get_ylim()[1], 11))\n",
    "\n",
    "    if len(models_dict) > 3:\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    else:\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if failed_models:\n",
    "        print(\"\\n[WARNING] The following models failed during execution:\")\n",
    "        for model_name in failed_models:\n",
    "            print(f\" - {model_name}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb61808c-34fa-47e8-93c2-6cf5b40e54f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_psnr_vs_epsilon_dataloader(\n",
    "    models_dict,\n",
    "    dev_loader,\n",
    "    max_epsilon=10,\n",
    "    num_points=5,\n",
    "    steps=10,\n",
    "    norm = \"linf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e59338-4828-460d-9629-9c2c0bd5b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "knee_loader = create_data_loaders(args,True)\n",
    "len(knee_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65f7a4-ac6d-4641-afec-7f0acb18eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(args_Unet_with_trajectory_learning, 1, Unet_with_trajectory_learning, dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb1f04-af8d-4936-afaa-40df8f8db356",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(args_Unet_with_trajectory_learning, 1, Unet_with_trajectory_learning_with_PGD_Train_3norm, dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0012df06-1c41-4aad-9941-e29da2f074d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Unet_with_trajectory_learning.module.get_trajectory()\n",
    "v, a = get_vel_acc(x)\n",
    "acc_loss = torch.sqrt(torch.sum(torch.pow(F.softshrink(a, args_Unet_with_trajectory_learning.a_max).abs() + 1e-8, 2)))\n",
    "vel_loss = torch.sqrt(torch.sum(torch.pow(F.softshrink(v, args_Unet_with_trajectory_learning.v_max).abs() + 1e-8, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac865d9-3871-418b-976e-1169b44359f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_loss, acc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f0f60-48ff-45da-86f3-94afd42a5256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm  \n",
    "\n",
    "optimizer = torch.optim.Adam(Unet_with_trajectory_learning_with_constant_noise_10000_l1_2.module.parameters(), lr=1e-1)\n",
    "\n",
    "a_max, v_max = args_Unet_with_trajectory_learning.a_max, args_Unet_with_trajectory_learning.v_max\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100, verbose=True)\n",
    "\n",
    "with tqdm(desc=\"Training Iterations\", ncols=100, leave=True) as pbar:\n",
    "    iteration = 0\n",
    "    while True: \n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        x = Unet_with_trajectory_learning_with_constant_noise_10000_l1_2.module.get_trajectory()\n",
    "\n",
    "        v, a = get_vel_acc(x)\n",
    "\n",
    "        # Compute acceleration and velocity losses\n",
    "        acc_loss = torch.sqrt(torch.sum(torch.pow(F.softshrink(a, a_max).abs() + 1e-8, 2)))\n",
    "        vel_loss = torch.sqrt(torch.sum(torch.pow(F.softshrink(v, v_max).abs() + 1e-8, 2)))\n",
    "\n",
    "        # Total loss\n",
    "        loss = acc_loss + vel_loss\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the progress bar description with current loss\n",
    "        pbar.set_postfix(loss=acc_loss.item())\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Check if loss is less than the desired threshold\n",
    "        if acc_loss.item() < 5*1e-5 and vel_loss.item() < 1e-5:\n",
    "            print(f\"acc_loss reached {acc_loss.item()} at iteration {iteration + 1}. Stopping training.\")\n",
    "            break\n",
    "\n",
    "        # Step the scheduler to adjust the learning rate based on the loss\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        # Increment the iteration\n",
    "        iteration += 1\n",
    "\n",
    "    print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c9964b-01d8-4820-87de-8d1cffbb8328",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trajectory(Unet_with_trajectory_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d5987-3013-4ba3-a3c2-119ac54a5c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(first_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e8429-0f7d-4858-954d-06710952ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(Unet_with_trajectory_learning(first_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce9553-93d3-40b6-b826-792c861aa085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "from fastmri.losses import SSIMLoss\n",
    "\n",
    "def pgd_attack_with_trajectory(model, input_tensor, target_tensor, epsilon, steps=20):\n",
    "    if input_tensor.dim() < 5:\n",
    "        input_tensor = input_tensor.view(*([1] * (5 - input_tensor.dim())), *input_tensor.shape)\n",
    "    \n",
    "    initial_trajectory = model.module.subsampling.x.detach().clone() \n",
    "    trajectory_param = model.module.subsampling.x\n",
    "    trajectory_param.requires_grad = True  \n",
    "    \n",
    "\n",
    "    output = model(input_tensor)\n",
    "    if output.shape != target_tensor.shape:\n",
    "        target_tensor = target_tensor.view_as(output)\n",
    "        \n",
    "    init_psnr = psnr(target_tensor.to('cpu').numpy(), output.cpu().detach().numpy())\n",
    "    lowest_psnr = float('inf')\n",
    "    best_trajectory = initial_trajectory.clone().data.detach()\n",
    "    \n",
    "    for step in range(steps):\n",
    "        if model.module.subsampling.x.grad != None:\n",
    "            model.module.subsampling.x.grad.zero_()\n",
    "        \n",
    "        output = model(input_tensor)\n",
    "        if output.shape != target_tensor.shape:\n",
    "            target_tensor = target_tensor.view_as(output)\n",
    "            \n",
    "        loss = F.l1_loss(output.to(\"cpu\"), target_tensor.to(\"cpu\"))\n",
    "        psnr_value = psnr(target_tensor.to('cpu').numpy(), output.cpu().detach().numpy())\n",
    "        \n",
    "        if psnr_value < lowest_psnr:\n",
    "            lowest_psnr = psnr_value\n",
    "            best_trajectory = model.module.subsampling.x.data.clone().detach()\n",
    "            difference = initial_trajectory - best_trajectory\n",
    "            \n",
    "        grad = torch.autograd.grad(loss, model.module.subsampling.x, retain_graph=True)[0]\n",
    "\n",
    "        trajectory = model.module.subsampling.x.data + grad.sign()  \n",
    "        \n",
    "        eta = torch.clamp(trajectory - initial_trajectory, min=-epsilon, max=epsilon)  \n",
    "\n",
    "        model.module.subsampling.x.data = initial_trajectory + eta \n",
    "        \n",
    "        trajectory = torch.clamp(trajectory, min=-160, max=160)\n",
    "    print(init_psnr, \"-->\", lowest_psnr)\n",
    "    print(type(initial_trajectory))\n",
    "    model.module.subsampling.x = torch.nn.Parameter(initial_trajectory, requires_grad=True)  # Restore original\n",
    "    return best_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3073e35-2803-4565-a82c-5fb62a5fd899",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Unet_with_trajectory_learning.module.training:\n",
    "    print(\"The model is in evaluation mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e79359-8b5b-48da-a67d-18fc4c0421db",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_grads = {\n",
    "    name: param.grad.clone() if param.grad is not None else None\n",
    "    for name, param in Unet_with_trajectory_learning.named_parameters()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e8d34-2cd8-430d-97b0-20cc72da7f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unet_with_trajectory_learning.module.eval()\n",
    "initial_trajectory = Unet_with_trajectory_learning.module.get_trajectory().data.detach().clone()\n",
    "epsilon = 1\n",
    "noise_level = epsilon * torch.prod(torch.tensor(initial_trajectory.shape))\n",
    "noise_type = \"ones\"\n",
    "diff = pgd_attack_with_trajectory(Unet_with_trajectory_learning, first_input, first_target, epsilon, steps=10) - Unet_with_trajectory_learning.module.get_trajectory().data\n",
    "print(diff.max(), diff.min())\n",
    "print(torch.norm(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e146a-10cb-42c5-a435-6cf2de7ac778",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        grad_diff = torch.norm(param.grad - initial_grads[name])\n",
    "        if grad_diff > 1e-6:  # Arbitrary threshold to check if gradients changed\n",
    "            print(f\"Gradients for {name} have changed!\")\n",
    "        else:\n",
    "            print(f\"Gradients for {name} have not changed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8934ec8e-9c6c-4722-b054-f34f0a8ce426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_poly(t):\n",
    "    tt = [None for _ in range(4)]\n",
    "    tt[0] = 1\n",
    "    for i in range(1, 4):\n",
    "        tt[i] = tt[i - 1] * t\n",
    "    A = torch.tensor([\n",
    "        [1, 0, -3, 2],\n",
    "        [0, 1, -2, 1],\n",
    "        [0, 0, 3, -2],\n",
    "        [0, 0, -1, 1]\n",
    "    ], dtype=tt[-1].dtype)\n",
    "    return [\n",
    "        sum(A[i, j] * tt[j] for j in range(4))\n",
    "        for i in range(4)]\n",
    "\n",
    "def interp(x, y, xs):\n",
    "    m = (y[1:] - y[:-1]) / (x[1:] - x[:-1])\n",
    "    m = torch.cat([m[[0]], (m[1:] + m[:-1]) / 2, m[[-1]]])\n",
    "    I = P.searchsorted(x[1:].detach().cpu(), xs.detach().cpu())\n",
    "    I = torch.clamp(I, 0, len(x) - 2)  # Ensure I is within bounds\n",
    "    dx = (x[I + 1] - x[I])\n",
    "    hh = h_poly((xs - x[I]) / dx)\n",
    "    return hh[0] * y[I] + hh[1] * m[I] * dx + hh[2] * y[I + 1] + hh[3] * m[I + 1] * dx\n",
    "\n",
    "def interp_full(x, interp_gap):\n",
    "    \"\"\"Interpolates the entire dataset at a higher resolution.\"\"\"\n",
    "    t = torch.arange(0, x.shape[1], device=x.device).float()\n",
    "    t1 = t[::interp_gap]\n",
    "    x_short = x[:, ::interp_gap, :]\n",
    "\n",
    "    x_new = x.clone()  # Create a copy to store the interpolated values\n",
    "    for shot in range(x_short.shape[0]):\n",
    "        for d in range(2):\n",
    "            x_new[shot, :, d] = interp(t1, x_short[shot, :, d], t)\n",
    "\n",
    "    return x_new\n",
    "\n",
    "x = torch.rand(16, 320, 2)  \n",
    "interp_gap = 2\n",
    "x_interp1 = interp_full(x, interp_gap)\n",
    "x_interp2 = interp_full(x_interp1, interp_gap)\n",
    "print(torch.norm(x_interp1 - x_interp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a68b08b-5f68-4835-8085-d0093b5a9d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a348877f-1579-45eb-94f4-6124b7a0a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(1,1 , 320, 320).cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a60c4-cd34-481f-81de-ccce685befb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unet_with_trajectory_learning.module.reconstruction_model(input_tensor).data  -  Unet_with_trajectory_learning.module.reconstruction_model(input_tensor).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da68f1ec-d88a-4125-a911-3f3802b00bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(input, x, interp_gap):\n",
    "    if interp_gap > 1:\n",
    "        t = torch.arange(0, x.shape[1], device=x.device).float()\n",
    "        t1 = t[::interp_gap]\n",
    "        x_short = x[:, ::interp_gap, :]\n",
    "        for shot in range(x_short.shape[0]):\n",
    "            for d in range(2):\n",
    "                x.data[shot, :, d] = interp(t1, x_short[shot, :, d], t)\n",
    "\n",
    "    x_full = x.reshape(-1,2)\n",
    "    input = input.permute(0, 1, 4, 2, 3)\n",
    "    sub_ksp = nufft(input, x_full)\n",
    "    output = nufft_adjoint(sub_ksp, x_full, input.shape)\n",
    "    output = output.permute(0, 1, 3, 4, 2)\n",
    "    return output, sub_ksp, x_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c679291-fd7a-4dd0-9e76-8b606b3f56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "input_tensor = torch.randn(1, 1, 320, 320, 2).cuda()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6860202f-b48b-4528-a86d-ca94221ea8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = input_tensor.permute(0, 1, 4, 2, 3).shape\n",
    "output1, sub_ksp1, x_full1 = forward(input_tensor, Unet_with_trajectory_learning.module.get_trajectory().data, 10)\n",
    "output2, sub_ksp2, x_full2 = forward(input_tensor, Unet_with_trajectory_learning.module.get_trajectory().data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46504884-c9e2-4194-9438-fe7368aa6af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = nufft_adjoint(sub_ksp1, x_full1, input_tensor.permute(0, 1, 4, 2, 3).shape)\n",
    "output2 = nufft_adjoint(sub_ksp1, x_full1, input_tensor.permute(0, 1, 4, 2, 3).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf787252-416d-4eb9-b8e4-8b0e52c747f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 - output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72630fa6-c17e-44af-9fd8-90e3e73dc71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 - output2, sub_ksp1 - sub_ksp2, x_full1 - x_full2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e54206-9e19-4f63-b5b2-89e2a8528d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "output1 = Unet_with_trajectory_learning(input_tensor)\n",
    "output3 = Unet_with_trajectory_learning(input_tensor)\n",
    "\n",
    "print(f\"Output1: {output1.data}\")\n",
    "print(f\"Output3: {output3.data}\")\n",
    "\n",
    "are_outputs_identical = torch.allclose(output1, output3, atol=1e-6)\n",
    "print(f\"Are outputs identical? {are_outputs_identical}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a73d6-ac2c-48b9-8c9c-5bc1c69cce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_trajectory = Unet_with_trajectory_learning.module.subsampling.x.detach().clone()\n",
    "l1_noise = torch.randn_like(initial_trajectory) if noise_type == \"random\" else torch.ones_like(initial_trajectory)\n",
    "l1_norm = torch.norm(l1_noise, p=1)\n",
    "l1_noise_scaled = noise_level * l1_noise / l1_norm \n",
    "\n",
    "l2_noise = torch.randn_like(initial_trajectory) if noise_type == \"random\" else torch.ones_like(initial_trajectory)\n",
    "l2_norm = torch.norm(l2_noise)\n",
    "l2_noise_scaled = noise_level * l2_noise / l2_norm \n",
    "\n",
    "noisy_trajectory_l1 = initial_trajectory + l1_noise_scaled\n",
    "noisy_trajectory_l1 = torch.clamp(noisy_trajectory_l1, min=-160, max=160)\n",
    "\n",
    "noisy_trajectory_l2 = initial_trajectory + l2_noise_scaled\n",
    "noisy_trajectory_l2 = torch.clamp(noisy_trajectory_l2, min=-160, max=160)\n",
    "\n",
    "print(torch.norm(noisy_trajectory_l1 - initial_trajectory))\n",
    "\n",
    "Unet_with_trajectory_learning.module.subsampling.x.data = noisy_trajectory_l1\n",
    "output3 = Unet_with_trajectory_learning(first_input)\n",
    "\n",
    "if output.shape != first_target.shape:\n",
    "    first_target = first_target.view_as(output)\n",
    "psnr_l1_noise = psnr(first_target.to('cpu').numpy(), output.cpu().detach().numpy())\n",
    "\n",
    "Unet_with_trajectory_learning.module.subsampling.x.data = noisy_trajectory_l2\n",
    "output = Unet_with_trajectory_learning(first_input)\n",
    "if output.shape != first_target.shape:\n",
    "    first_target = first_target.view_as(output)\n",
    "psnr_l2_noise = psnr(first_target.to('cpu').numpy(), output.cpu().detach().numpy())\n",
    "\n",
    "print(\"psnr_l1_noise: \", psnr_l1_noise, \" psnr_l2_noise: \", psnr_l2_noise)\n",
    "Unet_with_trajectory_learning.module.get_trajectory().data = initial_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cbcf6d-7c42-49da-bea0-c4bd5c9c37bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output3.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c28b22-40e7-474d-895f-5c86a896ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "output1.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e774ef96-a674-4d47-80a2-4ab96b0f1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbaa01a-1a5b-41dc-9a69-ce32111501cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5092cdc1-7555-40ff-93b6-33693b432e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9420feff-c659-4849-85c6-9575dcfbfbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(args_Unet_with_trajectory_learning, 1, Unet_with_trajectory_learning, dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a1718-35c4-4837-842d-42045e2aa987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(args, model, data_loader, adv_norm = \"l1\", epsilon = None):\n",
    "    losses = []\n",
    "    psnr_l = []\n",
    "    ssim_l = []\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    progress_bar = tqdm(enumerate(data_loader), total=len(data_loader), desc=f\"Evaluating\")\n",
    "\n",
    "    for iter, data in progress_bar:\n",
    "        init_trajectory = model.module.get_trajectory().data\n",
    "        input, target, mean, std, norm = data\n",
    "        input = input.to(args.device)\n",
    "        resolution = target.shape[-1]\n",
    "        target = target.to(args.device)\n",
    "        if epsilon != None:\n",
    "            noisy_trajectory_l1 = pgd_attack_with_trajectory(model, input, target, epsilon, norm = adv_norm, steps=10)\n",
    "            model.module.get_trajectory().data = noisy_trajectory_l1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input.unsqueeze(1))\n",
    "            recons = output.to('cpu').squeeze(1).view(target.shape)\n",
    "            recons = recons.squeeze()\n",
    "            if output.shape != target.shape:\n",
    "                target = target.view_as(output)\n",
    "\n",
    "            loss = F.l1_loss(output, target)\n",
    "            losses.append(loss.item()) \n",
    "\n",
    "            target = target.view(-1, resolution, resolution)\n",
    "            recons = recons.view(target.shape)\n",
    "\n",
    "            psnr_value = psnr(target.to('cpu').numpy(), recons.numpy())\n",
    "            ssim_value = ssim(target.to('cpu').numpy(), recons.numpy())\n",
    "            psnr_l.append(psnr_value)\n",
    "            ssim_l.append(ssim_value)\n",
    "\n",
    "            # Update tqdm progress bar with current metrics\n",
    "            progress_bar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"PSNR\": f\"{psnr_value:.2f}\",\n",
    "                \"SSIM\": f\"{ssim_value:.4f}\"\n",
    "            })\n",
    "            model.module.get_trajectory().data = init_trajectory\n",
    "\n",
    "    print(f'PSNR: {np.mean(psnr_l):.2f}  {np.std(psnr_l):.2f}, SSIM: {np.mean(ssim_l):.4f}  {np.std(ssim_l):.4f}')\n",
    "\n",
    "    x = model.module.get_trajectory()\n",
    "    v, a = get_vel_acc(x)\n",
    "    acc_loss = torch.sqrt(torch.sum(torch.pow(F.softshrink(a, args.a_max), 2)))\n",
    "    vel_loss = torch.sqrt(torch.sum(torch.pow(F.softshrink(v, args.v_max), 2)))\n",
    "    rec_loss = np.mean(losses)\n",
    "\n",
    "    return rec_loss, np.mean(psnr_l), np.mean(ssim_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c1c3e29fbecc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "\n",
    "def apply_noise(noise_type = \"random\"):\n",
    "    noise_levels = np.logspace(3, 10, 11)\n",
    "    for model_name, file in checkpoint_files.items():\n",
    "        _, model, _, args = load_model(file)\n",
    "        args.sample_rate = 0.25\n",
    "        args.batch_size = 10\n",
    "        test_loader = create_data_loaders(args)\n",
    "        initial_trajectory = model.module.subsampling.x.clone()\n",
    "        results = []\n",
    "\n",
    "        for noise_level in noise_levels:\n",
    "            print(\"noise_level: \", noise_level)\n",
    "            model.module.subsampling.x.data = initial_trajectory\n",
    "            #_, psnr_l1_adv, ssim_l1_adv = evaluate(args, model, test_loader, \"l1\", noise_level)\n",
    "            #_, psnr_l2_adv, ssim_l2_adv = evaluate(args, model, test_loader, \"l2\", noise_level)\n",
    "            \n",
    "            l1_noise = torch.randn_like(initial_trajectory) if noise_type == \"random\" else torch.ones_like(initial_trajectory)\n",
    "            l1_norm = torch.norm(l1_noise, p=1)\n",
    "            l1_noise_scaled = noise_level * l1_noise / l1_norm \n",
    "\n",
    "            l2_noise = torch.randn_like(initial_trajectory) if noise_type == \"random\" else torch.ones_like(initial_trajectory)\n",
    "            l2_norm = torch.norm(l2_noise)\n",
    "            l2_noise_scaled = noise_level * l2_noise / l2_norm \n",
    "\n",
    "            noisy_trajectory_l1 = initial_trajectory + l1_noise_scaled\n",
    "            noisy_trajectory_l1 = torch.clamp(noisy_trajectory_l1, min=-160, max=160)\n",
    "\n",
    "            noisy_trajectory_l2 = initial_trajectory + l2_noise_scaled\n",
    "            noisy_trajectory_l2 = torch.clamp(noisy_trajectory_l2, min=-160, max=160)\n",
    "            \n",
    "            noise_level_l1 = torch.norm(noisy_trajectory_l1 - initial_trajectory, p=1)\n",
    "            noise_level_l2 = torch.norm(noisy_trajectory_l2 - initial_trajectory, p=2)\n",
    "            \n",
    "            print(\"noise_level_l1: \", noise_level_l1, \"noise_level_l2: \", noise_level_l2)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.module.subsampling.x.data = noisy_trajectory_l1\n",
    "            dev_loss_l1, noisy_psnr_l1, noisy_ssim_l1, noisy_psnr_l1_s, noisy_ssim_l1_s = evaluate(args, 1, model, test_loader)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.module.subsampling.x.data = noisy_trajectory_l2\n",
    "            dev_loss_l2, noisy_psnr_l2, noisy_ssim_l2, noisy_psnr_l2_s, noisy_ssim_l2_s = evaluate(args, 1,model, test_loader)\n",
    "            \n",
    "            results.append({\n",
    "                'noise_level_l1': float(noise_level_l1.item()),\n",
    "                'noise_level_l2': float(noise_level_l2.item()),\n",
    "                'psnr_l1': (noisy_psnr_l1, noisy_psnr_l1_s),\n",
    "                'ssim_l1': (noisy_ssim_l1, noisy_ssim_l1_s),\n",
    "                'psnr_l2': (noisy_psnr_l2, noisy_psnr_l2_s),\n",
    "                'ssim_l2': (noisy_ssim_l2, noisy_ssim_l2_s),\n",
    "            })\n",
    "\n",
    "\n",
    "        with open(f'results_{model_name}_{noise_type}.json', 'w') as f:\n",
    "            json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad7cef3-5aef-4f90-b36e-757000e4f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_noise(\"ones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3785d96e2075294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def load_json(file_name):\n",
    "    \"\"\"Load the JSON data from a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_name, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Manually enter the JSON file names with labels for the legend\n",
    "json_files = [\n",
    "    #(\"results_Unet_with_trajectory_learning_ones.json\", \"Baseline\"),\n",
    "    (\"results_Unet_with_trajectory_learning_with_constant_noise_10000_l1_0.2_ones.json\", \"Noise 0.2\"),\n",
    "    (\"results_Unet_with_trajectory_learning_with_constant_noise_10000_l1_0.5_ones.json\", \"Noise 0.5\"),\n",
    "    (\"results_Unet_with_trajectory_learning_with_linear_noise_1e6_l1_1_ones.json\", \"Baseline2\")\n",
    "]\n",
    "\n",
    "# Lists to store data from all files\n",
    "file_data = []\n",
    "\n",
    "# Load data from each file\n",
    "for file_name, label in json_files:\n",
    "    data = load_json(file_name)\n",
    "    if not data:\n",
    "        print(f\"Skipping {file_name} due to empty or invalid data.\")\n",
    "        continue\n",
    "    num_entries = len(data)\n",
    "    print(f\"{file_name} contains {num_entries} entries.\")\n",
    "\n",
    "    # Extract noise levels and metrics\n",
    "    noise_l1 = np.array([entry.get('noise_level_l1', np.nan) for entry in data])\n",
    "    noise_l2 = np.array([entry.get('noise_level_l2', np.nan) for entry in data])\n",
    "    psnr_l1_means = np.array([entry.get('psnr_l1', [np.nan, np.nan])[0] for entry in data])\n",
    "    psnr_l1_stds = np.array([entry.get('psnr_l1', [np.nan, np.nan])[1] for entry in data])\n",
    "    psnr_l2_means = np.array([entry.get('psnr_l2', [np.nan, np.nan])[0] for entry in data])\n",
    "    psnr_l2_stds = np.array([entry.get('psnr_l2', [np.nan, np.nan])[1] for entry in data])\n",
    "    ssim_l1_means = np.array([entry.get('ssim_l1', [np.nan, np.nan])[0] for entry in data])\n",
    "    ssim_l1_stds = np.array([entry.get('ssim_l1', [np.nan, np.nan])[1] for entry in data])\n",
    "    ssim_l2_means = np.array([entry.get('ssim_l2', [np.nan, np.nan])[0] for entry in data])\n",
    "    ssim_l2_stds = np.array([entry.get('ssim_l2', [np.nan, np.nan])[1] for entry in data])\n",
    "\n",
    "    file_data.append({\n",
    "        'label': label,\n",
    "        'noise_l1': noise_l1,\n",
    "        'noise_l2': noise_l2,\n",
    "        'psnr_l1_means': psnr_l1_means,\n",
    "        'psnr_l1_stds': psnr_l1_stds,\n",
    "        'psnr_l2_means': psnr_l2_means,\n",
    "        'psnr_l2_stds': psnr_l2_stds,\n",
    "        'ssim_l1_means': ssim_l1_means,\n",
    "        'ssim_l1_stds': ssim_l1_stds,\n",
    "        'ssim_l2_means': ssim_l2_means,\n",
    "        'ssim_l2_stds': ssim_l2_stds\n",
    "    })\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# PSNR L1 vs Noise L1\n",
    "plt.subplot(2, 2, 1)\n",
    "for data in file_data:\n",
    "    plt.errorbar(data['noise_l1'], data['psnr_l1_means'], yerr=data['psnr_l1_stds'], \n",
    "                 fmt='o-', capsize=5, label=data['label'])\n",
    "plt.xscale('log')\n",
    "plt.title('PSNR L1 vs Noise L1')\n",
    "plt.xlabel('Noise Level L1 (log scale)')\n",
    "plt.ylabel('PSNR L1 Mean')\n",
    "plt.legend()\n",
    "\n",
    "# PSNR L2 vs Noise L2\n",
    "plt.subplot(2, 2, 2)\n",
    "for data in file_data:\n",
    "    plt.errorbar(data['noise_l2'], data['psnr_l2_means'], yerr=data['psnr_l2_stds'], \n",
    "                 fmt='o-', capsize=5, label=data['label'])\n",
    "plt.xscale('log')\n",
    "plt.title('PSNR L2 vs Noise L2')\n",
    "plt.xlabel('Noise Level L2 (log scale)')\n",
    "plt.ylabel('PSNR L2 Mean')\n",
    "plt.legend()\n",
    "\n",
    "# SSIM L1 vs Noise L1\n",
    "plt.subplot(2, 2, 3)\n",
    "for data in file_data:\n",
    "    plt.errorbar(data['noise_l1'], data['ssim_l1_means'], yerr=data['ssim_l1_stds'], \n",
    "                 fmt='o-', capsize=5, label=data['label'])\n",
    "plt.xscale('log')\n",
    "plt.title('SSIM L1 vs Noise L1')\n",
    "plt.xlabel('Noise Level L1 (log scale)')\n",
    "plt.ylabel('SSIM L1 Mean')\n",
    "plt.legend()\n",
    "\n",
    "# SSIM L2 vs Noise L2\n",
    "plt.subplot(2, 2, 4)\n",
    "for data in file_data:\n",
    "    plt.errorbar(data['noise_l2'], data['ssim_l2_means'], yerr=data['ssim_l2_stds'], \n",
    "                 fmt='o-', capsize=5, label=data['label'])\n",
    "plt.xscale('log')\n",
    "plt.title('SSIM L2 vs Noise L2')\n",
    "plt.xlabel('Noise Level L2 (log scale)')\n",
    "plt.ylabel('SSIM L2 Mean')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f329af-5f44-4dbf-8c90-4f8cd316e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def load_json(file_name):\n",
    "    \"\"\"Load the JSON data from a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_name, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Manually enter the JSON file names with labels for the legend\n",
    "json_files = [\n",
    "    #(\"results_Unet_with_trajectory_learning_ones.json\", \"Baseline\"),\n",
    "    (\"results_baseline__ones.json\", \"results_baseline__ones\"),\n",
    "    (\"results_baseline_plus_constant_noise_from_10000000_with_p___0.5__ones.json\", \"10000000_with_p___0.5__ones\"),\n",
    "    (\"results_baseline_plus_constant_noise_from_10000000_with_p___1__ones.json\", \"10000000_with_p___1__ones\"),\n",
    "]\n",
    "\n",
    "# Lists to store data from all files\n",
    "file_data = []\n",
    "\n",
    "# Load data from each file\n",
    "for file_name, label in json_files:\n",
    "    data = load_json(file_name)\n",
    "    if not data:\n",
    "        print(f\"Skipping {file_name} due to empty or invalid data.\")\n",
    "        continue\n",
    "    num_entries = len(data)\n",
    "    print(f\"{file_name} contains {num_entries} entries.\")\n",
    "\n",
    "    # Extract noise levels and metrics\n",
    "    noise_l1 = np.array([entry.get('noise_level_l1', np.nan) for entry in data])\n",
    "    noise_l2 = np.array([entry.get('noise_level_l2', np.nan) for entry in data])\n",
    "    psnr_l1_means = np.array([entry.get('psnr_l1', [np.nan, np.nan])[0] for entry in data])\n",
    "    psnr_l1_stds = np.array([entry.get('psnr_l1', [np.nan, np.nan])[1] for entry in data])\n",
    "    psnr_l2_means = np.array([entry.get('psnr_l2', [np.nan, np.nan])[0] for entry in data])\n",
    "    psnr_l2_stds = np.array([entry.get('psnr_l2', [np.nan, np.nan])[1] for entry in data])\n",
    "    ssim_l1_means = np.array([entry.get('ssim_l1', [np.nan, np.nan])[0] for entry in data])\n",
    "    ssim_l1_stds = np.array([entry.get('ssim_l1', [np.nan, np.nan])[1] for entry in data])\n",
    "    ssim_l2_means = np.array([entry.get('ssim_l2', [np.nan, np.nan])[0] for entry in data])\n",
    "    ssim_l2_stds = np.array([entry.get('ssim_l2', [np.nan, np.nan])[1] for entry in data])\n",
    "\n",
    "    file_data.append({\n",
    "        'label': label,\n",
    "        'noise_l1': noise_l1,\n",
    "        'noise_l2': noise_l2,\n",
    "        'psnr_l1_means': psnr_l1_means,\n",
    "        'psnr_l1_stds': psnr_l1_stds,\n",
    "        'psnr_l2_means': psnr_l2_means,\n",
    "        'psnr_l2_stds': psnr_l2_stds,\n",
    "        'ssim_l1_means': ssim_l1_means,\n",
    "        'ssim_l1_stds': ssim_l1_stds,\n",
    "        'ssim_l2_means': ssim_l2_means,\n",
    "        'ssim_l2_stds': ssim_l2_stds\n",
    "    })\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# PSNR L1 vs Noise L1\n",
    "plt.subplot(2, 2, 1)\n",
    "for data in file_data:\n",
    "    plt.errorbar(data['noise_l1'], data['psnr_l1_means'], yerr=data['psnr_l1_stds'], \n",
    "                 fmt='o-', capsize=5, label=data['label'])\n",
    "plt.xscale('log')\n",
    "plt.title('PSNR L1 vs Noise L1')\n",
    "plt.xlabel('Noise Level L1 (log scale)')\n",
    "plt.ylabel('PSNR L1 Mean')\n",
    "plt.legend()\n",
    "\n",
    "# PSNR L2 vs Noise L2\n",
    "plt.subplot(2, 2, 2)\n",
    "for data in file_data:\n",
    "    plt.errorbar(data['noise_l2'], data['psnr_l2_means'], yerr=data['psnr_l2_stds'], \n",
    "                 fmt='o-', capsize=5, label=data['label'])\n",
    "plt.xscale('log')\n",
    "plt.title('PSNR L2 vs Noise L2')\n",
    "plt.xlabel('Noise Level L2 (log scale)')\n",
    "plt.ylabel('PSNR L2 Mean')\n",
    "plt.legend()\n",
    "\n",
    "# SSIM L1 vs Noise L1\n",
    "plt.subplot(2, 2, 3)\n",
    "for data in file_data:\n",
    "    plt.errorbar(data['noise_l1'], data['ssim_l1_means'], yerr=data['ssim_l1_stds'], \n",
    "                 fmt='o-', capsize=5, label=data['label'])\n",
    "plt.xscale('log')\n",
    "plt.title('SSIM L1 vs Noise L1')\n",
    "plt.xlabel('Noise Level L1 (log scale)')\n",
    "plt.ylabel('SSIM L1 Mean')\n",
    "plt.legend()\n",
    "\n",
    "# SSIM L2 vs Noise L2\n",
    "plt.subplot(2, 2, 4)\n",
    "for data in file_data:\n",
    "    plt.errorbar(data['noise_l2'], data['ssim_l2_means'], yerr=data['ssim_l2_stds'], \n",
    "                 fmt='o-', capsize=5, label=data['label'])\n",
    "plt.xscale('log')\n",
    "plt.title('SSIM L2 vs Noise L2')\n",
    "plt.xlabel('Noise Level L2 (log scale)')\n",
    "plt.ylabel('SSIM L2 Mean')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c48cc-6e15-4dd9-9360-f83c30c0203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def load_json(file_name):\n",
    "    \"\"\"Load the JSON data from a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_name, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Manually enter the JSON file names with labels for the legend\n",
    "json_files = [\n",
    "    #(\"results_Unet_with_trajectory_learning_ones.json\", \"Baseline\"),\n",
    "    (\"results_baseline__ones.json\", \"results_baseline__ones\"),\n",
    "    (\"results_baseline_plus_linear_noise_from_1e-06_to_1000000000_with_p___0.5__ones.json\",\"linear_noise_to_1000000000_with_p___0.5__ones\"),\n",
    "    (\"results_baseline_plus_linear_noise_from_1e-08_to_100000000_with_p___0.5__ones.json\",\"linear_noise_to_100000000_with_p___0.5__ones\")\n",
    "]\n",
    "\n",
    "# Lists to store data from all files\n",
    "file_data = []\n",
    "\n",
    "# Load data from each file\n",
    "for file_name, label in json_files:\n",
    "    data = load_json(file_name)\n",
    "    if not data:\n",
    "        print(f\"Skipping {file_name} due to empty or invalid data.\")\n",
    "        continue\n",
    "    num_entries = len(data)\n",
    "    print(f\"{file_name} contains {num_entries} entries.\")\n",
    "\n",
    "    # Extract noise levels and metrics\n",
    "    noise_l1 = np.array([entry.get('noise_level_l1', np.nan) for entry in data])\n",
    "    noise_l2 = np.array([entry.get('noise_level_l2', np.nan) for entry in data])\n",
    "    psnr_l1_means = np.array([entry.get('psnr_l1', [np.nan, np.nan])[0] for entry in data])\n",
    "    psnr_l1_stds = np.array([entry.get('psnr_l1', [np.nan, np.nan])[1] for entry in data])\n",
    "    psnr_l2_means = np.array([entry.get('psnr_l2', [np.nan, np.nan])[0] for entry in data])\n",
    "    psnr_l2_stds = np.array([entry.get('psnr_l2', [np.nan, np.nan])[1] for entry in data])\n",
    "    ssim_l1_means = np.array([entry.get('ssim_l1', [np.nan, np.nan])[0] for entry in data])\n",
    "    ssim_l1_stds = np.array([entry.get('ssim_l1', [np.nan, np.nan])[1] for entry in data])\n",
    "    ssim_l2_means = np.array([entry.get('ssim_l2', [np.nan, np.nan])[0] for entry in data])\n",
    "    ssim_l2_stds = np.array([entry.get('ssim_l2', [np.nan, np.nan])[1] for entry in data])\n",
    "\n",
    "    file_data.append({\n",
    "        'label': label,\n",
    "        'noise_l1': noise_l1,\n",
    "        'noise_l2': noise_l2,\n",
    "        'psnr_l1_means': psnr_l1_means,\n",
    "        'psnr_l1_stds': psnr_l1_stds,\n",
    "        'psnr_l2_means': psnr_l2_means,\n",
    "        'psnr_l2_stds': psnr_l2_stds,\n",
    "        'ssim_l1_means': ssim_l1_means,\n",
    "        'ssim_l1_stds': ssim_l1_stds,\n",
    "        'ssim_l2_means': ssim_l2_means,\n",
    "        'ssim_l2_stds': ssim_l2_stds\n",
    "    })\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# PSNR L1 vs Noise L1\n",
    "plt.subplot(2, 2, 1)\n",
    "for data in file_data:\n",
    "    plt.errorbar(data['noise_l1'], data['psnr_l1_means'], yerr=data['psnr_l1_stds'], \n",
    "                 fmt='o-', capsize=5, label=data['label'])\n",
    "plt.xscale('log')\n",
    "plt.title('PSNR L1 vs Noise L1')\n",
    "plt.xlabel('Noise Level L1 (log scale)')\n",
    "plt.ylabel('PSNR L1 Mean')\n",
    "plt.legend()\n",
    "\n",
    "# PSNR L2 vs Noise L2\n",
    "plt.subplot(2, 2, 2)\n",
    "for data in file_data:\n",
    "    plt.errorbar(data['noise_l2'], data['psnr_l2_means'], yerr=data['psnr_l2_stds'], \n",
    "                 fmt='o-', capsize=5, label=data['label'])\n",
    "plt.xscale('log')\n",
    "plt.title('PSNR L2 vs Noise L2')\n",
    "plt.xlabel('Noise Level L2 (log scale)')\n",
    "plt.ylabel('PSNR L2 Mean')\n",
    "plt.legend()\n",
    "\n",
    "# SSIM L1 vs Noise L1\n",
    "plt.subplot(2, 2, 3)\n",
    "for data in file_data:\n",
    "    plt.errorbar(data['noise_l1'], data['ssim_l1_means'], yerr=data['ssim_l1_stds'], \n",
    "                 fmt='o-', capsize=5, label=data['label'])\n",
    "plt.xscale('log')\n",
    "plt.title('SSIM L1 vs Noise L1')\n",
    "plt.xlabel('Noise Level L1 (log scale)')\n",
    "plt.ylabel('SSIM L1 Mean')\n",
    "plt.legend()\n",
    "\n",
    "# SSIM L2 vs Noise L2\n",
    "plt.subplot(2, 2, 4)\n",
    "for data in file_data:\n",
    "    plt.errorbar(data['noise_l2'], data['ssim_l2_means'], yerr=data['ssim_l2_stds'], \n",
    "                 fmt='o-', capsize=5, label=data['label'])\n",
    "plt.xscale('log')\n",
    "plt.title('SSIM L2 vs Noise L2')\n",
    "plt.xlabel('Noise Level L2 (log scale)')\n",
    "plt.ylabel('SSIM L2 Mean')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b83fdf5-7c5f-4a9f-820c-a7f6be88722b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def load_json(file_name):\n",
    "    \"\"\"Load the JSON data from a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_name, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Manually enter the JSON file names with labels for the legend\n",
    "json_files = [\n",
    "    #(\"results_Unet_with_trajectory_learning_ones.json\", \"Baseline\"),\n",
    "    (\"results_baseline__ones.json\", \"results_baseline__random\"),\n",
    "    (\"results_baseline_plus_linear_noise_from_1e-06_to_1000000000_with_p___0.5__random.json\",\"linear_noise_to_1000000000_with_p___0.5__random\"),\n",
    "    (\"results_baseline_plus_linear_noise_from_1e-06_to_1000000000_with_p___0.1__random.json\",\"linear_noise_to_1000000000_with_p___0.1__random\"),\n",
    "    (\"results_baseline_plus_linear_noise_from_1e-08_to_100000000_with_p___0.5__random.json\",\"linear_noise_to_100000000_with_p___0.5__random\"),\n",
    "]\n",
    "\n",
    "# Lists to store data from all files\n",
    "file_data = []\n",
    "\n",
    "# Load data from each file\n",
    "for file_name, label in json_files:\n",
    "    data = load_json(file_name)\n",
    "    if not data:\n",
    "        print(f\"Skipping {file_name} due to empty or invalid data.\")\n",
    "        continue\n",
    "    num_entries = len(data)\n",
    "    print(f\"{file_name} contains {num_entries} entries.\")\n",
    "\n",
    "    # Extract noise levels and metrics\n",
    "    noise_l1 = np.array([entry.get('noise_level_l1', np.nan) for entry in data])\n",
    "    noise_l2 = np.array([entry.get('noise_level_l2', np.nan) for entry in data])\n",
    "    psnr_l1_means = np.array([entry.get('psnr_l1', [np.nan, np.nan])[0] for entry in data])\n",
    "    psnr_l1_stds = np.array([entry.get('psnr_l1', [np.nan, np.nan])[1] for entry in data])\n",
    "    psnr_l2_means = np.array([entry.get('psnr_l2', [np.nan, np.nan])[0] for entry in data])\n",
    "    psnr_l2_stds = np.array([entry.get('psnr_l2', [np.nan, np.nan])[1] for entry in data])\n",
    "    ssim_l1_means = np.array([entry.get('ssim_l1', [np.nan, np.nan])[0] for entry in data])\n",
    "    ssim_l1_stds = np.array([entry.get('ssim_l1', [np.nan, np.nan])[1] for entry in data])\n",
    "    ssim_l2_means = np.array([entry.get('ssim_l2', [np.nan, np.nan])[0] for entry in data])\n",
    "    ssim_l2_stds = np.array([entry.get('ssim_l2', [np.nan, np.nan])[1] for entry in data])\n",
    "\n",
    "    file_data.append({\n",
    "        'label': label,\n",
    "        'noise_l1': noise_l1,\n",
    "        'noise_l2': noise_l2,\n",
    "        'psnr_l1_means': psnr_l1_means,\n",
    "        'psnr_l1_stds': psnr_l1_stds,\n",
    "        'psnr_l2_means': psnr_l2_means,\n",
    "        'psnr_l2_stds': psnr_l2_stds,\n",
    "        'ssim_l1_means': ssim_l1_means,\n",
    "        'ssim_l1_stds': ssim_l1_stds,\n",
    "        'ssim_l2_means': ssim_l2_means,\n",
    "        'ssim_l2_stds': ssim_l2_stds\n",
    "    })\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# PSNR L1 vs Noise L1\n",
    "plt.subplot(2, 2, 1)\n",
    "for data in file_data:\n",
    "    plt.errorbar(data['noise_l1'], data['psnr_l1_means'], yerr=data['psnr_l1_stds'], \n",
    "                 fmt='o-', capsize=5, label=data['label'])\n",
    "plt.xscale('log')\n",
    "plt.title('PSNR L1 vs Noise L1')\n",
    "plt.xlabel('Noise Level L1 (log scale)')\n",
    "plt.ylabel('PSNR L1 Mean')\n",
    "plt.legend()\n",
    "\n",
    "# PSNR L2 vs Noise L2\n",
    "plt.subplot(2, 2, 2)\n",
    "for data in file_data:\n",
    "    plt.errorbar(data['noise_l2'], data['psnr_l2_means'], yerr=data['psnr_l2_stds'], \n",
    "                 fmt='o-', capsize=5, label=data['label'])\n",
    "plt.xscale('log')\n",
    "plt.title('PSNR L2 vs Noise L2')\n",
    "plt.xlabel('Noise Level L2 (log scale)')\n",
    "plt.ylabel('PSNR L2 Mean')\n",
    "plt.legend()\n",
    "\n",
    "# SSIM L1 vs Noise L1\n",
    "plt.subplot(2, 2, 3)\n",
    "for data in file_data:\n",
    "    plt.errorbar(data['noise_l1'], data['ssim_l1_means'], yerr=data['ssim_l1_stds'], \n",
    "                 fmt='o-', capsize=5, label=data['label'])\n",
    "plt.xscale('log')\n",
    "plt.title('SSIM L1 vs Noise L1')\n",
    "plt.xlabel('Noise Level L1 (log scale)')\n",
    "plt.ylabel('SSIM L1 Mean')\n",
    "plt.legend()\n",
    "\n",
    "# SSIM L2 vs Noise L2\n",
    "plt.subplot(2, 2, 4)\n",
    "for data in file_data:\n",
    "    plt.errorbar(data['noise_l2'], data['ssim_l2_means'], yerr=data['ssim_l2_stds'], \n",
    "                 fmt='o-', capsize=5, label=data['label'])\n",
    "plt.xscale('log')\n",
    "plt.title('SSIM L2 vs Noise L2')\n",
    "plt.xlabel('Noise Level L2 (log scale)')\n",
    "plt.ylabel('SSIM L2 Mean')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f4f20-45c5-4e22-933f-5959dfeb86ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python mpilot-py310",
   "language": "python",
   "name": "mpilot-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
